{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"CTSM on HiPerGator","text":"<p>Documentation for running the Community Terrestrial Systems Model (CTSM) on HiPerGator at the University of Florida.</p>"},{"location":"#what-is-ctsm","title":"What is CTSM?","text":"<p>CTSM is the land component of the Community Earth System Model (CESM). It simulates terrestrial ecosystem processes including:</p> <ul> <li>Vegetation dynamics and carbon cycling</li> <li>Hydrology and water balance</li> <li>Energy balance and heat transfer</li> <li>Biogeochemistry (carbon-nitrogen cycling)</li> </ul> <p>CTSM can be run standalone (without ocean/atmosphere coupling) for land-focused research.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>If you're new to this documentation:</p> <ol> <li>Onboarding - HiPerGator basics and environment setup</li> <li>Quick Start - Get CTSM running step by step</li> <li>Case Workflow - Detailed case configuration</li> </ol> <p>For deeper understanding, see Prerequisites and Fork Reference.</p>"},{"location":"#reference-fork","title":"Reference Fork","text":"<p>We maintain forks of CTSM with HiPerGator-specific modifications. You can use these as a starting point for your own installation, or fork them to make additional changes.</p> Repository Branch Purpose cdevaneprugh/CTSM <code>uf-ctsm5.3.085</code> CTSM with tool fixes cdevaneprugh/ccs_config_cesm <code>uf-hipergator</code> HiPerGator machine config <p>Version Notice</p> <p>These forks are based on CTSM 5.3.085. Other versions may require different fixes.</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"Section Content Installation Prerequisites, fork setup, building tools Running CTSM Case workflow, single-point runs, output configuration Research Group-specific research context (placeholders) Reference Fork modifications, external resources Archive Historical CESM documentation"},{"location":"#key-resources","title":"Key Resources","text":"<ul> <li>CTSM GitHub Wiki - Official development docs</li> <li>CTSM Technical Note - Model science documentation</li> <li>CTSM User's Guide - Setup and running</li> <li>CIME Documentation - Build and case management</li> <li>DiscussCESM Forums - Community support</li> </ul>"},{"location":"glossary/","title":"Glossary","text":"<p>Key terms for CTSM development on HiPerGator.</p>"},{"location":"glossary/#models-components","title":"Models &amp; Components","text":"<p>CTSM - Community Terrestrial Systems Model. The land model, designed to run standalone or as part of CESM. GitHub</p> <p>CESM - Community Earth System Model. Fully-coupled global climate model. CTSM is its land component.</p> <p>CLM - Community Land Model. The land component within CTSM that models vegetation, hydrology, carbon cycling, etc.</p> <p>CIME - Common Infrastructure for Modeling the Earth (pronounced \"SEAM\"). Provides the case control system for configuring, building, and running Earth system models.</p> <p>FATES - Functionally Assembled Terrestrial Ecosystem Simulator. A vegetation demographic model that can be coupled to CLM for more detailed vegetation dynamics. FATES GitHub</p>"},{"location":"glossary/#tools-utilities","title":"Tools &amp; Utilities","text":"<p>cprnc - Tool for comparing NetCDF files. Used to verify model output. Each group builds their own (see Prerequisites).</p> <p>git-fleximod - CTSM's tool for managing submodules. Replaces the older <code>manage_externals</code> used by CESM.</p> <p>mksurfdata - Tool for generating surface datasets (fsurdat files) for CTSM. Requires PIO library.</p> <p>PIO (ParallelIO) - Parallel I/O library required for mksurfdata. Each group builds their own (see Prerequisites).</p>"},{"location":"glossary/#configuration-data","title":"Configuration &amp; Data","text":"<p>CLM_USRDAT - Resolution setting for user-provided datasets. Used with subset data for single-point runs.</p> <p>Compset - Component set. A predefined combination of model components and configurations. Format: <code>YYYYA_B_C_D_E_F</code> where letters represent different components. Example: <code>I1850Clm60BgcCrop</code> = Pre-industrial (1850), CLM 6.0 with BGC and crops.</p> <p>Namelist - Fortran configuration file format. Key files: <code>user_nl_clm</code>, <code>user_nl_datm_streams</code>.</p> <p>PFT - Plant Functional Type. Categories of vegetation (e.g., broadleaf deciduous tree, C3 grass, corn) that share similar characteristics. CTSM uses PFTs to represent vegetation at each grid point.</p> <p>Surface data (fsurdat) - NetCDF file containing land surface characteristics (soil, vegetation, topography).</p>"},{"location":"glossary/#output-files","title":"Output Files","text":"<p>History files - Model output files containing time-varying data (<code>.h0.nc</code>, <code>.h1.nc</code>, etc.). Configured via <code>hist_*</code> namelist variables. Use for analysis and plotting.</p> <p>Restart files - Checkpoint files (<code>.r.nc</code>) that capture the complete model state. Used to continue simulations from a specific point. Required for branch runs and spinup continuation.</p>"},{"location":"glossary/#case-directories","title":"Case Directories","text":"<p>CASEROOT - Case configuration directory. Contains XML settings, namelists, scripts.</p> <p>EXEROOT - Build directory where compiled code lives.</p> <p>RUNDIR - Runtime directory containing output, restarts, logs.</p> <p>DOUT_S_ROOT - Archive directory for completed output.</p>"},{"location":"glossary/#run-types","title":"Run Types","text":"<p>startup - Fresh start with cold initialization.</p> <p>branch - Exact continuation from a restart file. Preserves bit-for-bit answers.</p> <p>hybrid - New start initialized from another case's restart file. Allows configuration changes.</p>"},{"location":"glossary/#spinup","title":"Spinup","text":"<p>AD spinup - Accelerated Decomposition spinup. First phase (~200 years) with accelerated soil carbon turnover.</p> <p>Post-AD spinup - Second phase (several hundred years) with normal turnover rates to reach equilibrium.</p>"},{"location":"glossary/#infrastructure","title":"Infrastructure","text":"<p>ccs_config - Repository containing machine configurations for CIME. Our fork adds HiPerGator support.</p> <p>ESMF - Earth System Modeling Framework. Provides coupling infrastructure.</p> <p>NUOPC - National Unified Operational Prediction Capability. Layer built on ESMF for coupling standards.</p> <p>SCRIP - Spherical Coordinate Remapping and Interpolation Package. Used for creating mapping files.</p>"},{"location":"glossary/#hipergator-specific","title":"HiPerGator Specific","text":"<p>HPG - HiPerGator. UF's supercomputer.</p> <p>lmod - Module system for managing software environments.</p> <p>SLURM - Job scheduler for submitting work to compute nodes.</p> <p>QOS - Quality of Service. Defines resource limits for job queues.</p>"},{"location":"onboarding/","title":"Onboarding","text":"<p>This page provides essential HiPerGator information for running CTSM. For comprehensive Linux training, see the resources at the bottom.</p>"},{"location":"onboarding/#hipergator-basics","title":"HiPerGator Basics","text":"<p>HiPerGator is UF's supercomputer - a cluster of connected nodes that share resources. You interact with it via the command line.</p>"},{"location":"onboarding/#key-concepts","title":"Key Concepts","text":"Concept Description Login nodes Where you land when you SSH in. For light tasks only (editing, file management) Compute nodes Where jobs run. Access via SLURM scheduler SLURM Job scheduler - submits work to compute nodes lmod Module system for loading software (compilers, libraries)"},{"location":"onboarding/#storage-locations","title":"Storage Locations","text":"Path Purpose Notes <code>/home/$USER/</code> Config files, small files 40GB quota, backed up <code>/blue/&lt;group&gt;/$USER/</code> Active projects, code, data Group allocation, fast NVME <code>/orange/&lt;group&gt;/</code> Archival storage Slow - don't use for active work <p>Replace <code>&lt;group&gt;</code> with your research group's allocation (e.g., <code>gerber</code>, <code>biology</code>, etc.).</p>"},{"location":"onboarding/#module-environment","title":"Module Environment","text":"<p>CTSM requires specific modules. We use a saved module collection:</p> <pre><code># Load the CTSM module collection\nmodule restore ctsm-modules\n\n# Verify modules are loaded\nmodule list\n</code></pre> <p>The collection includes: GCC 14.2.0, OpenMPI 5.0.7, NetCDF, HDF5, ESMF 8.8.1, CMake, Python 3.12</p>"},{"location":"onboarding/#recommended-shortcuts","title":"Recommended Shortcuts","text":"<p>These environment variables can save typing. Add them to your <code>~/.bashrc</code> and customize for your setup:</p> <pre><code># Your workspace (customize &lt;group&gt;)\nexport BLUE=\"/blue/&lt;group&gt;/$USER\"\nexport CASES=\"$BLUE/cases\"\n\n# Your CTSM installation (after cloning)\nexport CTSMROOT=\"$BLUE/ctsm5.3\"\nexport CIME_SCRIPTS=\"$CTSMROOT/cime/scripts\"\n\n# Your group's input data location\nexport INPUT_DATA=\"/blue/&lt;group&gt;/earth_models/inputdata\"\n</code></pre> <p>After editing, reload: <code>source ~/.bashrc</code></p> <p>Tip</p> <p>The exact paths depend on where you install CTSM and where your group stores input data. These are suggestions, not requirements.</p>"},{"location":"onboarding/#slurm-basics","title":"SLURM Basics","text":"<p>Submit jobs to compute nodes - never run heavy computation on login nodes.</p> <pre><code># Check your running jobs\nsqueue -u $USER\n\n# Check group jobs (replace &lt;group&gt;)\nsqueue -A &lt;group&gt;\n\n# Cancel a job\nscancel &lt;job_id&gt;\n\n# View job details\nscontrol show job &lt;job_id&gt;\n</code></pre>"},{"location":"onboarding/#group-qos-limits","title":"Group QOS Limits","text":"<p>Each group has its own resource limits. Check your group's QoS:</p> <pre><code>showQos -A &lt;group&gt;\n</code></pre> <p>This shows your group's CPU, memory, and GPU limits. CTSM cases will fail if they request more resources than your QoS allows.</p>"},{"location":"onboarding/#useful-commands","title":"Useful Commands","text":"<pre><code># Show HiPerGator environment variables\nenv | grep HPC | sort\n\n# View directory structure\ntree | less\n\n# Check disk usage (customize path)\ndu -sh /blue/&lt;group&gt;/$USER/\n</code></pre>"},{"location":"onboarding/#learning-resources","title":"Learning Resources","text":""},{"location":"onboarding/#hipergator-documentation","title":"HiPerGator Documentation","text":"<ul> <li>Getting Started</li> <li>Training Videos</li> <li>SLURM Scheduling</li> <li>Module System</li> </ul>"},{"location":"onboarding/#linux-command-line","title":"Linux Command Line","text":"<ul> <li>Linux Journey - Comprehensive tutorials</li> <li>UF Linux Training - HiPerGator-focused</li> <li>The Art of Command Line - Reference guide</li> </ul>"},{"location":"onboarding/#text-editors","title":"Text Editors","text":"<p>The default editor is often <code>vim</code>. If you accidentally open it:</p> <ul> <li>Press <code>Esc</code>, then type <code>:q!</code> and press Enter to quit without saving</li> <li>Or press <code>Esc</code>, then <code>:wq</code> and Enter to save and quit</li> </ul> <p>Alternatives: <code>nano</code> (simpler), <code>emacs</code></p>"},{"location":"archive/","title":"Archived Documentation","text":"<p>This folder contains documentation that is no longer actively maintained but preserved for historical reference.</p>"},{"location":"archive/#why-these-files-are-archived","title":"Why These Files Are Archived","text":"File Reason <code>cesm-porting.md</code> CESM-specific installation - we now focus exclusively on CTSM <code>cesm-usage.md</code> CESM workflow documentation - CTSM has different workflows <code>ctsm-doc-comparison.md</code> Historical analysis of documentation discrepancies <code>intro-to-earth-models.md</code> Contained E3SM content and excessive CESM detail"},{"location":"archive/#current-focus","title":"Current Focus","text":"<p>This documentation now focuses on CTSM (Community Terrestrial Systems Model) running on HiPerGator at the University of Florida.</p> <p>For current documentation, return to the Home page.</p>"},{"location":"archive/#notes","title":"Notes","text":"<ul> <li>CTSM is the land component of CESM, but can be run standalone</li> <li>We use a fork-based approach with HiPerGator-specific modifications</li> <li>Our fork: <code>github.com/cdevaneprugh/CTSM</code> branch <code>uf-ctsm5.3.085</code></li> </ul>"},{"location":"archive/README-FIRST/","title":"Earth Model Documentation","text":""},{"location":"archive/README-FIRST/#overview","title":"Overview","text":"<p>The primary goal of this documentation is to serve as a guide for porting the CESM, CTSM, and E3SM Earth models to HiPerGator. I follow the official documentation for each Earth model, and add steps specific to HiPerGator as needed. Additionally, I'm including a section that can serve as an introduction to using Linux, which many students have limited exposure to. Ultimately, the goal at UF is to have several Earth models installed in some shared directory on HiPerGator. Until then this document can serve as a guide to do an install for your research group.</p>"},{"location":"archive/README-FIRST/#important-notes-for-porting-earth-models","title":"Important Notes for Porting Earth Models","text":"<ul> <li>Start with <code>shared-utils.md</code>: This file is essential for understanding directory structures and module dependencies before beginning the porting process.</li> <li>Ensure the correct environment is loaded: Use <code>module restore esm_gnu_env</code> before working with models to avoid dependency issues.</li> <li>Follow the porting documentation step by step: Missing configurations can cause failures, so adhere closely to the provided instructions.</li> <li>Group specific setup: This documentation details how these Earth models were ported specifically for the Gerber research group. Your needs may be different.</li> </ul>"},{"location":"archive/README-FIRST/#directory-tree","title":"Directory Tree","text":"<pre><code>docs/\n\u251c\u2500\u2500 glossary.md                 # Glossary of terms related to Earth system models and HiPerGator\n\u251c\u2500\u2500 intro-to-earth-models.md    # Overview of Earth system models and their porting process\n\u251c\u2500\u2500 linux-overview.md           # Introduction to Linux and HiPerGator usage\n\u251c\u2500\u2500 porting/                    \n\u2502   \u251c\u2500\u2500 cesm-porting.md         # Steps to port CESM to HiPerGator\n\u2502   \u251c\u2500\u2500 ctsm-porting.md         # Steps to port CTSM to HiPerGator\n\u2502   \u251c\u2500\u2500 forking-ctsm.md         # Steps to fork CTSM and associated submodules (not required)\n\u2502   \u251c\u2500\u2500 mksurfdata-fixes.md     # Steps to fix bugs in the CTSM/tools directory (not required)\n\u2502   \u2514\u2500\u2500 shared-utils.md         # *READ FIRST* - Shared directory structure and module setup\n\u2514\u2500\u2500 usage/                      \n    \u251c\u2500\u2500 cesm-usage.md           # Instructions for running CESM\n    \u251c\u2500\u2500 ctsm-doc-comparison.md  # Clarifications on CTSM documentation inconsistencies\n    \u2514\u2500\u2500 ctsm-usage.md           # Instructions for running CTSM\n</code></pre>"},{"location":"archive/README-FIRST/#file-descriptions","title":"File Descriptions","text":""},{"location":"archive/README-FIRST/#general-documentation","title":"General Documentation","text":"<ul> <li><code>glossary.md</code>: Contains definitions of common terms used throughout the documentation.</li> <li><code>intro-to-earth-models.md</code>: Provides background information on Earth system models, their structure, and functionality.</li> <li><code>linux-overview.md</code>: Introduces Linux basics, command-line usage, and HiPerGator-specific commands.</li> </ul>"},{"location":"archive/README-FIRST/#porting-guides-porting","title":"Porting Guides (<code>porting/</code>)","text":"<ul> <li><code>shared-utils.md</code> (Read this first)</li> <li>Describes the shared directory structure for Earth models.</li> <li>Explains required modules and module collections for proper setup.</li> <li>Provides an overview of CIME configuration files.</li> <li><code>cesm-porting.md</code></li> <li>Covers the steps to install and configure CESM on HiPerGator.</li> <li>Includes downloading, setting up external components, and running regression tests.</li> <li><code>ctsm-porting.md</code></li> <li>Details the process for setting up CTSM on HiPerGator.</li> <li>Includes configuring the <code>mksurfdata</code> tool and setting up required dependencies.</li> <li><code>forking-ctsm.md</code></li> <li>The process for forking CTSM. </li> <li>You can duplicate these steps if you wish to fork CTSM or our UF repo for your own modifications.</li> <li>Not required reading if you are only interested in porting.</li> <li><code>mksurfdata-fixes.md</code></li> <li>A record of all the bug fixes and modifications in <code>$CTSM/tools</code>.</li> </ul>"},{"location":"archive/README-FIRST/#usage-guides-usage","title":"Usage Guides (<code>usage/</code>)","text":"<ul> <li><code>cesm-usage.md</code></li> <li>Instructions for creating, building, and running CESM cases.</li> <li>Details about adjusting job resources, troubleshooting, and running single-point cases.</li> <li><code>ctsm-usage.md</code></li> <li>Steps to configure and run CTSM simulations.</li> <li>Details on setting up single-point or regional grid cases.</li> <li><code>ctsm-doc-comparison.md</code></li> <li>Clarifies inconsistencies in CTSM documentation.</li> <li>Summarizes missing tools and alternative approaches for dataset creation.</li> </ul>"},{"location":"archive/README-FIRST/#additional-resources","title":"Additional Resources","text":"<p>For further support, refer to the official documentation: - CESM: CESM Documentation - CTSM: CTSM Documentation</p>"},{"location":"archive/cesm-porting/","title":"Porting CESM","text":""},{"location":"archive/cesm-porting/#introduction","title":"Introduction","text":"<p>CESM has two primary releases, the current development release (v2.2.2 at the time of this writing), and the production release (v2.1.5). We will be using the production release. I am following the CESM documentation, as well as the CIME porting documentation while adding the steps needed to get this working on HiPerGator.</p>"},{"location":"archive/cesm-porting/#downloading-the-code","title":"Downloading the Code","text":"<p>While the repository is not that large, it should be put on the <code>/blue</code> drive for fast access and not in your <code>home</code> directory. It is completely up to the user and group how the file structure is organized. We decided to have a shared directory called <code>/earth_models</code> that contains the source code for the different models, as well as the input data directory (which can get extremely large). We have planned to move the input data directory to <code>orange</code> at some point in the future. Remember to make sure that all users in the group have read/write privileges to this directory.</p> <p>Follow directions in the CESM documentation to clone the repository and checkout the external components.</p> <pre><code># cd into the directory you want cesm installed\ncd /blue/$GROUP/earth_models\n\n# clone the cesm repository\ngit clone -b release-cesm2.1.5 https://github.com/ESCOMP/CESM.git cesm2.1.5\n\n# cd into your cesm directory\ncd cesm2.1.5\n\n# download the external components\n./manage_externals/checkout_externals\n\n# check that the components were installed correctly\n./manage_externals/checkout_externals -S\n</code></pre> <p>The last command should output something like:</p> <pre><code>Processing externals description file : Externals.cfg\nProcessing externals description file : Externals_CLM.cfg\nProcessing externals description file : Externals_POP.cfg\nProcessing externals description file : Externals_CISM.cfg\nChecking status of externals: clm, fates, ptclm, mosart, ww3, cime, cice, pop, cvmix, marbl, cism, source_cism, rtm, cam,\n    ./cime\n    ./components/cam\n    ./components/cice\n    ./components/cism\n    ./components/cism/source_cism\n    ./components/clm\n    ./components/clm/src/fates\n    ./components/clm/tools/PTCLM\n    ./components/mosart\n    ./components/pop\n    ./components/pop/externals/CVMix\n    ./components/pop/externals/MARBL\n    ./components/rtm\n    ./components/ww3\n</code></pre> <p>If there were issues with any of the components, you would see an error:</p> <pre><code>e-  ./components/clm\n</code></pre> <p>By default, CESM downloads the most recent version of the CIME code, which unfortunately causes some bugs. We can fix that by checking out an older, more stable version.</p> <pre><code>cd cime\ngit pull origin maint-5.6\ngit checkout maint-5.6\n</code></pre> <p>If you run <code>./checkout_externals -S</code> after changing the CIME branch, it may show an error that CIME is not using the correct version. You can ignore this.</p>"},{"location":"archive/cesm-porting/#regression-testing","title":"Regression Testing","text":"<p>Once the config files are setup, we need to run regression tests to ensure things are working correctly. The regression tests script will test various parameters of the Earth model in isolation, then send a dozen or two small cases to the scheduler to be run. This script is not resource intensive and can be run from a login node.</p> <p>You'll need python to run the script. </p> <pre><code># load python\nmodule load python/3.12\n\n# go to the location of the regression tests\ncd /blue/GROUP/earth_models/CESM/cime/scripts/tests\n</code></pre> <p>You can run the script with several options. Use the <code>--help</code> tag when running the script for the full list of them. For example, if you want to run the script and test the intel compilers, using a specific output directory, you could do something like:</p> <pre><code>./scripts_regression_tests.py --compiler gcc --test-root PATH/TO/TEST/OUTPUT\n</code></pre>"},{"location":"archive/cesm-porting/#ensemble-consistency-testing","title":"Ensemble Consistency Testing","text":"<p>Follow the guide here in order to complete these tests for scientific validation. You'll just need to change the case output location, machine, and compiler name to reflect your setup. I should note that to run the script to create these tests we have to load an older version of python.</p> <pre><code>module load python-core/2.7.14\n</code></pre>"},{"location":"archive/cesm-usage/","title":"CESM Usage","text":"<ol> <li>Basics</li> </ol> <p>1.1 Recommended Reading</p> <p>1.2 File Structure</p> <p>1.3 Creating, Building, and Running a Case</p> <p>1.4 Example</p> <ol> <li>Single Point Cases</li> </ol> <p>2.1 Best Practices</p> <p>2.2 Compset Testing</p> <ol> <li>Single Point With Spin Up</li> </ol>"},{"location":"archive/cesm-usage/#1-basics","title":"1. Basics","text":""},{"location":"archive/cesm-usage/#11-recommended-reading","title":"1.1 Recommended Reading","text":"<p>There are three pieces of documentation that I strongly suggest you read through to familiarize yourself with the process of creating cases on CESM.</p> <ol> <li>The quick start section of CESM's documentation.</li> <li>The Using the Case Control System section of the CIME documentation.</li> <li>The clm documentation, as the SWES department primarily uses <code>clm</code>, the land model component of CESM.</li> </ol>"},{"location":"archive/cesm-usage/#12-cesm-file-structure-on-hpg","title":"1.2 CESM File Structure on HPG","text":"<p>For the \"gerber\" group on HiPerGator, CESM has been installed in <code>/blue/gerber/earth_models/cesm2.1.5</code>. The scripts to create a new case or query information about case options are located in <code>/blue/gerber/earth_models/cesm2.1.5/cime/scripts</code>.</p> <p>If you leave the <code>machine_config.xml</code> at its default settings, and create a \"cases\" directory at <code>/blue/GROUP/USER/cases</code>, the directory tree at <code>/blue/GROUP/USER</code> should look something like:</p> <pre><code>.\n\u251c\u2500\u2500 cases\n\u2502   \u2514\u2500\u2500 EXAMPLE_CASE\n\u2514\u2500\u2500 earth_model_output\n    \u251c\u2500\u2500 cesm_baselines\n    \u251c\u2500\u2500 cime_output_root\n    \u2502   \u251c\u2500\u2500 archive\n    \u2502   \u2514\u2500\u2500 EXAMPLE_CASE\n    \u2502       \u251c\u2500\u2500 bld\n    \u2502       \u2514\u2500\u2500 run\n    \u2514\u2500\u2500 timings\n</code></pre>"},{"location":"archive/cesm-usage/#13-creating-building-and-running-a-case-on-hpg","title":"1.3 Creating, Building, and Running a Case on HPG","text":"<pre><code># cd to the cime scripts directory\ncd /blue/gerber/earth_models/cime/scripts\n\n# create your case, specifying the case location, compset, and resolution\n./create_newcase --case /blue/GROUP/USER/cases/EXAMPLE_CASE --compset COMPSET --res RESOLUTION\n</code></pre> <p>CIME will output a bunch of text, then say whether the the case was created successfully, and where it was created. If you are running a compset that is not scientifically validated, you will have to add the <code>--run-unsupported</code> option to your <code>create_newcase</code> command.</p> <pre><code># go to the case directory\ncd /blue/GROUP/USER/cases/case1\n</code></pre> <p>For the \"Gerber\" group, there are a few variables that we will most likely need to change. The first is the number of cores used by the model. Most compsets will request one (or several) nodes (128-512 cores) by default. Our research group only has access to 20 cores on our default queue, so we need to make sure we're under the QOS limit. We can do this with the <code>xmlchange</code> script.</p> <p>First check how many cores each component is requesting by running <code>./pelayout</code>, which will list out the individual components, along with what resources they are asking for. The variable we want to pay atention to is <code>NTASKS</code>. This corresponds to how many cores will be requested when the case is submitted.</p> <pre><code># change the number of cores to something more sensible\n./xmlchange NTASKS=8\n</code></pre> <p>This will take us down to using only 8 cores when we run the case.</p> <p>The downside of using fewer cores, is that we may have to run the case for longer on the compute node. We can extend the time requested by changeing the <code>JOB_WALLCLOCK_TIME</code> variable.</p> <pre><code># check the current setting\n./xmlquery JOB_WALLCLOCK_TIME\n\n# change the variable if needed\n./xmlchange JOB_WALLCLOCK_TIME=1:00:00\n</code></pre> <p>If you're unsure of the exact name of the varibale you want to check/change, you can list all the defined variables, then pipe to grep and search. <pre><code># running grep with the -i flag will ignore case distinction\n./xmlquery --listall | grep -i SEARCH_TERM\n</code></pre> Once you've set all of your variables, setup, build, and submit the case as usual. <pre><code>./case.setup\n./case.build\n./case.submit\n</code></pre></p> <p>If you setup and build the case but realize you need to change some variables before submitting, it's a good idea to clean the case before rebuilding. We can do this with something like:</p> <pre><code>./xmlchange SOME_VARIABLE\n\n./case.setup --clean\n./case.build --clean\n\n./case.setup\n./case.build\n./case.submit\n</code></pre>"},{"location":"archive/cesm-usage/#14-example-case","title":"1.4 Example Case","text":"<p>Here is an example of creating, building, and running a case with a compset typical of what we would use in the SWES department. The long name for our compset is <code>1850_DATM%CRUv7_CLM50%SP_SICE_SOCN_MOSART_CISM2%NOEVOLVE_SWAV</code> and the resolution we will be using is <code>f19_g17</code>.  While we can certainly use the long name for the compset, sometimes it's nicer to use the alias (assuming one is available). Here's a trick for finding your compset's alias.</p> <pre><code># cd to the cesm, cime scripts\ncd /blue/gerber/earth_models/cesm215/cime/scripts\n\n# use the query config script, then pipe it to grep and search the compset's long name\n./query_config --compsets all | grep 1850_DATM%CRUv7_CLM50%SP_SICE_SOCN_MOSART_CISM2%NOEVOLVE_SWAV\n</code></pre> <p>Which should output the following. <pre><code>I1850Clm50SpCru      : 1850_DATM%CRUv7_CLM50%SP_SICE_SOCN_MOSART_CISM2%NOEVOLVE_SWAV\n</code></pre></p> <p>Now we can create the case.</p> <pre><code># cd to the cesm, cime scripts\ncd /blue/gerber/earth_models/cesm215/cime/scripts\n\n# create the case using a sensible name, the compset alias, and desired resolution\n./create_newcase --case /blue/GROUP/USER/cases/EXAMPLE_CASE --compset I1850Clm50SpCru --res f19_g17\n\n# cd to the case directory\ncd /blue/GROUP/USER/cases/EXAMPLE_CASE\n\n# check the amount of cores being requested by default\n./xmlquery NTASKS\n\n# it's probably going to be higher than our QOS, so we need to change it along with extending the job time\n./xmlchange NTASKS=8,JOB_WALLCLOCK_TIME=1:00:00\n\n# setup the case\n./case.setup\n\n# check the submit script to make sure it's requesting the correct amount of resources\n./preview_run\n\n# if everything looks good, build the case (this can take a few minutes)\n./case.build\n\n# submit the case to the scheduler and run the experiment\n./case.submit\n</code></pre> <p>Check your UF email for updates from the <code>SLURM</code> scheduler. A case can fail for many reasons, most of which should be pretty obvious. If you accidentally requested more resources than your QOS allows, it will tell you in the email. If your case fails with an OOM (out of memory) error, try increasing the number of cores by changing the <code>NTASKS</code> variable. You may want to switch to your burst QOS sometimes. You can set this manually by changing the <code>JOB_QUEUE</code> variable (using the <code>xmlchange</code> script) to the name of your burst QOS. On hipergator your burst queue is your group name with \"-b\" appended. So the burst queue for the \"gerber\" group is gerber-b.</p>"},{"location":"archive/cesm-usage/#2-single-point-cases-in-cesm","title":"2. Single Point Cases in CESM","text":"<p>Following the instructions here, we can run the clm model on a single grid cell by specifying a latitude and longitude. However, the instructions on the clm website seem to be a bit outdated. CIME no longer supports the <code>-pts_lat</code> or <code>-pts_lon</code>  arguments with the <code>create_newcase</code> script, also multi-character arguments should begin with <code>--</code> rather than <code>-</code>.  We can still run on a single point by creating a new case, then changing the appropriate variables before building the executable.</p> <p>A Note on DATM_MODE source</p> <p>There are five modes used with CLM that specify the type of Meteorological data that\u2019s used. 1. CLMGSWP3 (this is the preferred meteorological data to use w/ CLM5) 2. CLMCRUNCEP (Use global NCEP forcing at half-degree resolution from CRU goes from 1900-2010. GSWP3 similar time period and spatial resolution). 3. CLM_QIAN (Deprecated. Use NCEP forcing at T62 resolution corrected by Qian et. al. goes from 1948-2004). 4. CLM1PT (Use the local meteorology from your specific tower site). 5. CPLHIST (This name may have changed. Use atmospheric data from a previous CESM simulation).</p> <pre><code># cd to cime/scripts \ncd /blue/gerber/earth_models/cime/scripts\n\n# find the shortname for the compset\n./query_config --compsets all | grep 1850_DATM%CRUv7_CLM50%SP_SICE_SOCN_MOSART_CISM2%NOEVOLVE_SWAV\n</code></pre> <p>Which outputs the following to the terminal.</p> <pre><code>I1850Clm50SpCru      : 1850_DATM%CRUv7_CLM50%SP_SICE_SOCN_MOSART_CISM2%NOEVOLVE_SWAV\n</code></pre> <p>To create our case we can do something like:</p> <pre><code>./create_newcase --case /blue/gerber/cdevaneprugh/cases/testPTS_OSBS --res f19_g17 --compset I1850Clm50SpCru\n\n# cd to the case directory\ncd /blue/gerber/cdevaneprugh/cases/testPTS_OSBS\n\n# change variables to run on a single point\n./xmlchange PTS_MODE=TRUE,PTS_LAT=29.7,PTS_LON=-82.0\n./xmlchange CLM_FORCE_COLDSTART=on,RUN_TYPE=startup\n\n# change variables to use a single core and adjust wall time\n./xmlchange NTASKS=1\n./xmlchange JOB_WALLCLOCK_TIME=1:00:00\n\n# setup, build, and submit the case as usual\n./case.setup\n./case.build\n./case.submit\n</code></pre> <p>This case will build and download input data, but fail during runtime with the following mpi error.  </p> <pre><code>MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD with errorcode 2.\n\nNOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.\nYou may or may not see output from other processes, depending on exactly when Open MPI kills them.\n</code></pre> <p>Following advice on my forum post, I tried a different compset which worked perfectly.</p> <p><pre><code># create case\n./create_newcase --case /blue/gerber/cdevaneprugh/cases/osbsPTSmod --compset 1850_DATM%CRUv7_CLM50%SP_SICE_SOCN_SROF_SGLC_SWAV --res f19_g17 --run-unsupported\ncd /blue/gerber/cdevaneprugh/cases/osbsPTSmod\n\n# change variables\n./xmlchange NTASKS=1,JOB_WALLCLOCK_TIME=1:00:00\n./xmlchange PTS_MODE=TRUE,PTS_LAT=29.7,PTS_LON=-82.0\n./xmlchange CLM_FORCE_COLDSTART=on,RUN_TYPE=startup\n\n# setup and build case like normal\n./case.setup\n./case.build\n\n# check input data\n./check_input_data\n\n# submit case\n./case.submit\n</code></pre> The next section will go into where we went wrong, and some things we can do to have successful runs in the future.</p>"},{"location":"archive/cesm-usage/#21-clm-best-practices-and-where-our-pts-run-went-wrong","title":"2.1 CLM Best Practices and Where Our PTS Run Went Wrong","text":"<p>The full comment from my post is:</p> <p>The error message in the cesm log is:</p> <p>MCT::m_SparseMatrixPlus:: FATAL--length of vector y different from row count of sMat.Length of y = 1 Number of rows in sMat = 13824 000.MCT(MPEU)::die.: from MCT::m_SparseMatrixPlus::initDistributed_()</p> <p>It seems like there must be mismatch between two of the datasets you are using. One seems to be single point (y=1) and the other is 1.9x2.5 (144x96=13824). I think it might be because the compset you are using has CISM and MOSART in it. The long name for that compset is:</p> <p>1850_DATM%CRUv7_CLM50%SP_SICE_SOCN_MOSART_CISM2%NOEVOLVE_SWAV</p> <p>Try specifying stubs for those, e.g.,</p> <p>1850_DATM%CRUv7_CLM50%SP_SICE_SOCN_SROF_SGLC_SWAV</p> <p>So the hypothesis is that any compset with MOSART or CISM will cause an issue in single point mode. Okay, so the easiest option is to just pick a compset we like, then specify stubs in the same way that was suggested in the post. The only problem is that doing this turns our compset into one that is not scientifically validated. This might not be an issue for our purposes. I also noticed an interesting thing in the clm documentation's best practices section.</p> <p>CLM5.0 includes BOTH the old CLM4.0, CLM4.5 physics AND the new CLM5.0 physics and you can toggle between those three. The \u201cstandard\u201d practice for CLM4.0 is to run with CN on, and with Qian atmospheric forcing. While the \u201cstandard\u201d practice for CLM4.5 is to run with BGC on, and CRUNCEP atmospheric forcing. And finally the \u201cstandard\u201d practice for CLM5.0 is to run with BGC and Prognostic Crop on, with the MOSART model for river routing, as well as the CISM ice sheet model, and using GSWP3 atmospheric forcing. \u201cBGC\u201d is the new CLM5.0 biogeochemistry and include CENTURY-like pools, vertical resolved carbon, as well as Nitrification and de-Nitrification</p> <p>So if it is best practice to use MOSART as well as CISM for clm5.0, maybe we can just use the clm4.0 or 4.5 physics instead. For a test, I'm going to look at the initial compsets Stefan gave to me, then see if I can find the equivalent one that is running clm4.0 or 4.5 and test those.</p> <p>Looking at the compsets, the one suggested in the forum post is effectively the clm4.0 version of what Stefan wanted, but with clm5.0 physics substituted (remember this doesn't follow best practices and is not scientifically validated). There are also plenty of scientifically valid compsets that don't use MOSART or CISM.</p> <p>It would be interesting to see if I can narrow down which is messing us up. I'll find a compset with only CISM, see if that runs globally and in single point mode, then repeat with a compset containing MOSART.</p> <p>With the abundance of compsets and resolutions, I'm going to need a better naming convention for my case directories. I think something like <code>compset.resolution.modifiers</code> would work. For example the name for our initial single point test (that failed) would be <code>I1850Clm50SpCru.f19_g17.PTS</code>.</p>"},{"location":"archive/cesm-usage/#22-compset-testing","title":"2.2 Compset Testing","text":"<p>Anytime we get an error that is something like:</p> <p>MCT::m_SparseMatrixPlus:: FATAL--length of vector y different from row count of sMat.Length of y = 1 Number of rows in sMat = 55296</p> <p>We can assume there is some issue with MOSART or CISM. The goal of running the following cases is to determine which is causing the error in single point mode. I did this by choosing similar compsets that use physics from clm4.0 4.5 and 5.0 as well as running a global and PTS version of each compset.</p> <p>Compset Long Name :   Compset Alias   :   Resolution Used</p> <p>1850_DATM%CRUv7_CLM40%SP_SICE_SOCN_RTM_SGLC_SWAV  :   I1850Clm40SpCruGs   :   f19_g17</p> <ol> <li> <p>Global: Success</p> </li> <li> <p>PTS: Success</p> </li> </ol> <p>1850_DATM%GSWP3v1_CLM45%CN_SICE_SOCN_RTM_CISM2%NOEVOLVE_SWAV  :   I1850Clm45Cn    :   f19_g17</p> <ol> <li> <p>Global: Success</p> </li> <li> <p>PTS: Fail </p> </li> </ol> <p>MCT::m_SparseMatrixPlus:: FATAL--length of vector y different from row count of sMat.Length of y = 1 Number of rows in sMat = 13824</p> <p>000.MCT(MPEU)::die.: from MCT::m_SparseMatrixPlus::initDistributed_()</p> <p>1850_DATM%CRUv7_CLM50%SP_SICE_SOCN_MOSART_CISM2%NOEVOLVE_SWAV :   I1850Clm50SpCru :   f19_g17</p> <ol> <li> <p>Global: Success</p> </li> <li> <p>PTS: Fail</p> </li> </ol> <p>MCT::m_SparseMatrixPlus:: FATAL--length of vector y different from row count of sMat.Length of y = 1 Number of rows in sMat = 13824</p> <p>000.MCT(MPEU)::die.: from MCT::m_SparseMatrixPlus::initDistributed_()</p> <p>1850_DATM%CRUv7_CLM50%SP_SICE_SOCN_SROF_SGLC_SWAV :   NONE    :   f19_g17</p> <ol> <li> <p>Global: Success</p> </li> <li> <p>PTS: Success</p> </li> </ol> <p>2000_DATM%GSWP3v1_CLM50%SP-VIC_SICE_SOCN_RTM_CISM2%NOEVOLVE_SWAV  :   I2000Clm50Vic   :   f09_g17</p> <ol> <li> <p>Global:</p> </li> <li> <p>PTS:</p> </li> </ol> <p>HIST_DATM%GSWP3v1_CLM40%SP_SICE_SOCN_RTM_SGLC_SWAV    :   IHistClm40SpGswGs   :   f09_g17</p> <ol> <li>Global: Success</li> <li>PTS: Success</li> </ol> <p>HIST_DATM%GSWP3v1_CLM45%SP_SICE_SOCN_RTM_SGLC_SWAV    :   IHistClm45SpGs  :   f09_g17</p> <ol> <li>Global: Fail (Out of Memory)</li> </ol> <p>Primary job  terminated normally, but 1 process returned a non-zero exit code. Per user-direction, the job has been aborted.</p> <ol> <li>PTS: Success</li> </ol> <p>HIST_DATM%QIA_CLM50%BGC_SICE_SOCN_MOSART_SGLC_SWAV    :   IHistClm50BgcQianGs :   f09_g17</p> <ol> <li>Global: Fail (Out of Memory)</li> </ol> <p>Primary job  terminated normally, but 1 process returned a non-zero exit code. Per user-direction, the job has been aborted.</p> <ol> <li>PTS: Success</li> </ol> <p>HIST_DATM%GSWP3v1_CLM50%SP_SICE_SOCN_MOSART_CISM2%NOEVOLVE_SWAV   :   IHistClm50Sp    :   f09_g17</p> <ol> <li>Global: Fail (Out of Memory)</li> </ol> <p>Primary job  terminated normally, but 1 process returned a non-zero exit code. Per user-direction, the job has been aborted.</p> <ol> <li>PTS: Fail</li> </ol> <p>MCT::m_SparseMatrixPlus:: FATAL--length of vector y different from row count of sMat.Length of y = 1 Number of rows in sMat = 55296</p> <p>000.MCT(MPEU)::die.: from MCT::m_SparseMatrixPlus::initDistributed_()</p> <p>Conclusion It looks like CISM is the issue. The Qian case (IHistClm50BgcQianGs) uses MOSART and the PTS mode case ran successfully.</p>"},{"location":"archive/cesm-usage/#3-single-point-with-spin-up-slides","title":"3. Single Point With Spin Up (Slides)","text":"<p>I'll go through the exercises in order here, and note any issues I ran in to or modifications I had to make.</p>"},{"location":"archive/cesm-usage/#exercise-4a-create-a-global-case","title":"Exercise 4a: Create a Global CaseHIST_DATM%GSWP3v1_CLM50%SP_SICE_SOCN_MOSART_SGLC_SWAV which is a modified version of the supported compset <code>IHistClm50Sp</code> that removes the CISM portion of the model. <pre><code># cd to cime scripts\ncd /blue/gerber/earth_models/cesm215/cime/scripts\n\n# create our case\n./create_newcase --case /blue/gerber/cdevaneprugh/cases/IHistClm50Sp_001 --compset HIST_DATM%GSWP3v1_CLM50%SP_SICE_SOCN_MOSART_SGLC_SWAV --res f09_g17 --run-unsupported\n\n# cd to case\ncd /blue/gerber/cdevaneprugh/cases/IHistClm50Sp_001\n\n./case.setup\n./preview_namelists\n\n# Look for the path to the surface data set and domain file\ncat CaseDocs/lnd_in\n</code></pre>","text":""},{"location":"archive/cesm-usage/#exercise-4b-generate-domain-and-surface-data-sets","title":"Exercise 4b: Generate Domain and Surface Data Sets <p>It looks like there's a script called <code>singlept</code> that we need access to. I think this was provided during the workshop on the NCAR server being used. Additionally, there is a <code>ncar_pylib</code> I need access to, it's possible this is available to download with <code>conda</code>. There was a list of many options that are changed in the <code>singlept</code> script but I can't find the variables they are setting when using <code>xmlquery</code> and looking in the <code>CaseDocs</code> directory.</p>","text":""},{"location":"archive/cesm-usage/#exercise-4c-create-a-new-case-for-the-single-point-run","title":"Exercise 4c: Create a New Case for the Single Point Run <pre><code># cd to cime scripts\ncd /blue/gerber/earth_models/cesm215/cime/scripts\n\n# create a new compset with a stub ice and river model\n./create_newcase --case /blue/gerber/cdevaneprugh/cases/ex_4c --compset 1850_DATM%CRUv7_CLM50%SP_SICE_SOCN_SROF_SGLC_SWAV --res f09_g17 --run-unsupported\n\n# go to case directory\ncd /blue/gerber/cdevaneprugh/cases/ex_4c\n</code></pre> <p>Following the guide there are many variables to change. Rather than list them all out, use the script <code>ready_clm_case</code> provided in this repository. Note: You should not change the <code>MPILIB</code> variable to <code>mpi-serial</code>. On our system, things seem to break if you do this. I am still unsure why. On page 30 of the slides they need the new domain files that we would have made in exercise 4b. We'll have to skip this part for now.</p> <p>It looks like all the variables were changed successfully, and the model was built. Unfortunately without those scripts from the workshop, we're sort of stuck. </p>","text":""},{"location":"archive/cesm-usage/#exercise-4d-single-point-bgc_ad","title":"Exercise 4d: Single Point BGC_AD <p>The xml variable changes are almost identical to exercise 4c, with one or two exceptions. Unfortunately, we run into the same problem here in that we need the scripts from exercise 4b.</p>","text":""},{"location":"archive/ctsm-doc-comparison/","title":"CTSM Documentation","text":"<p>Due to conflicting info within the online documentation, shipped documentation, and CESM forum, I feel it necessary to create a document to untangle and clarify the available information.</p>"},{"location":"archive/ctsm-doc-comparison/#clm-tools","title":"CLM Tools","text":"<p>From the online documentation.</p> <p>Located at <code>$CTSMROOT/tools</code>.</p> <p>The list of generally important scripts and programs are as follows:</p> <ol> <li>DOESNT EXIST<code>./mkmapgrids</code> to create SCRIP grid data files from old CLM format grid files that can then be used to create new CLM datasets (deprecated). </li> <li>There is also a NCL script (<code>./mkmapgrids/mkscripgrid.ncl</code>) to create SCRIP grid files for regular latitude/longitude grids.</li> <li>DOESNT EXIST<code>./mkmapdata</code> to create SCRIP mapping data file from SCRIP grid files (uses ESMF).</li> <li><code>mksurfdata_esmf</code> to create surface datasets from grid datasets (clm4_0 and CTSM1 versions).</li> <li>DOESNT EXIST<code>./mkprocdata_map</code> to interpolate output unstructured grids (such as the CAM HOMME dy-core \u201cne\u201d grids like ne30np4) into a 2D regular lat/long grid format that can be plotted easily. Can be used by either clm4_0 or CTSM1.</li> </ol>"},{"location":"archive/ctsm-doc-comparison/#creating-input-for-surface-dataset-generation","title":"Creating Input for Surface Dataset Generation","text":""},{"location":"archive/ctsm-doc-comparison/#generating-scrip-grid-files","title":"Generating SCRIP grid files","text":"<ol> <li> <p><code>mkmapdata.sh</code> needs SCRIP files which can be generated with <code>mkmapgrids</code>. Neither script exists</p> </li> <li> <p>There is a NCL script (<code>$CTSMROOT/tools/mkmapgrids/mkscripgrid.ncl</code>) to create regular latitude longitude regional or single-point grids at the resolution the user desires.</p> </li> </ol> <p>SCRIP grid files for all the standard model resolutions and the raw surface datasets have already been done and the files are in the XML database. Hence, this step doesn\u2019t need to be done \u2013 EXCEPT WHEN YOU ARE CREATING YOUR OWN GRIDS.</p> <p>Use <code>mknoocnmap.pl</code> in <code>$CTSMROOT/tools/mkmapdata</code> to create a regular latitude/longitude single-point or regional grid. It will create both the SCRIP grid file you need (using <code>$CTSMROOT/tools/mkmapgrids/mkscripgrid.ncl</code>) AND an identity mapping file assuming there is NO ocean in your grid domain.</p> <p><code>mkscripgrid.ncl</code> vs <code>mknoocnmap.pl</code>? </p> <p>What is an identity mapping file?</p>"},{"location":"archive/ctsm-doc-comparison/#creating-mapping-files-for-mksurfdata_esmf","title":"Creating mapping files for mksurfdata_esmf","text":"<p><code>mkmapdata.sh</code> uses the above SCRIP grid input files to create SCRIP mapping data files with ESMF. Script doesn't exist</p> <p>Theoretically it generates a list of maps from raw datasets that can be input to <code>mksurfdata_esmf</code>.</p>"},{"location":"archive/ctsm-doc-comparison/#creating-surface-datasets","title":"Creating Surface Datasets","text":"<p>If you are just creating a replacement file (not sure why you would need to) just use the relevant tool. If you are creating a set of files for a new resolution, you must create a SCRIP grid file first to use as an input for the other tools.</p> <p></p> <p>Basic idea:</p> <ol> <li>Generate your SCRIP grid files using <code>mkscripgrid.ncl</code>.</li> <li>Use <code>mkmapdata.sh</code> to create a list of SCRIP mapping files. What to use instead?</li> <li>The mapping files will tell <code>mksurfdata_esmf</code> how to map between the grid and raw datasets.</li> <li> <p>The output of <code>mksurfdata_esmf</code> is a surface dataset used for running the model.</p> </li> <li> <p>Enter the new datasets into the <code>build-namelist</code> XML database. This is optional, as you can enter things manually.</p> </li> </ol>"},{"location":"archive/ctsm-doc-comparison/#shipped-docs","title":"Shipped Docs","text":"<p>From <code>$CTSMROOT/tools/README</code></p>"},{"location":"archive/ctsm-doc-comparison/#process-sequence-to-create-input-datasets-needed-to-run-ctsm","title":"Process sequence to create input datasets needed to run CTSM","text":"<ol> <li>Create SCRIP grid files (if needed)</li> <li>Use <code>mknoocnmap.pl</code> to create SCRIP grid files and a mapping file for single point or regional cases.</li> </ol>"},{"location":"archive/ctsm-porting/","title":"Porting CTSM","text":"<p>Porting <code>CTSM</code> is largely the same process as <code>CESM</code>. You need to clone the repository, and download the external components. There are just a couple small differences in how this is achieved due to a different version of <code>CIME</code> being used. Additionally, we have a forked the <code>CTSM</code> repository, so the configuration files and necessary code fixes have already been handled.</p>"},{"location":"archive/ctsm-porting/#downloading-the-code","title":"Downloading the Code","text":"<pre><code># go the install directory. The Gerber group uses earth_models\ncd /blue/$GROUP/earth_models\n\n# clone the forked repo and cd into it\ngit clone https://github.com/cdevaneprugh/CTSM.git ctsm\ncd ctsm\n\n# checkout the desired release. for us it is the uf-ctsm branch\ngit checkout uf-ctsm5.3\n</code></pre> <p>Similar to <code>CESM</code> we need to download the externals. Instead of <code>checkout_externals</code> we use a script called <code>git-fleximod</code>.</p> <pre><code># in your ctsm root directory\n./bin/git-fleximod update\n</code></pre> <p>The configuration files are already provided in the forked repository. If you checked out a different branch, you will need to add configuration files as explained in <code>shared-utils.md</code>.</p>"},{"location":"archive/ctsm-porting/#regression-testing","title":"Regression Testing","text":"<p>If you'd like, you can run regression tests again. The <code>scripts_regression_tests.py</code> script is located in <code>$CTSMROOT/cime/CIME/tests</code>. The process is identical to as it was in <code>CESM</code>. I found these tests to randomly fail more often than the <code>CESM</code> ones. I'm not sure why this is, possibly because <code>CTSM</code> does not include all the components found in other Earth models.</p>"},{"location":"archive/ctsm-porting/#building-mksurfdata","title":"Building <code>mksurfdata</code>","text":"<p>The <code>mksurfdata</code> executable is used to generate fsurdat files for <code>CTSM</code>. If you want to use your own surface data, this tool must be built. It requires four libraries be properly configured before building. The first three (<code>MPI</code> <code>NetCDF</code> and <code>ESMF</code>) have already been configured on HiPerGator. The last library needed is <code>parallelio</code> (aka <code>PIO</code>). Instructions for building <code>parallelio</code> are found in <code>shared-utils.md</code>.</p> <p>To build the executable:</p> <p><pre><code># load the necessary modules\nmodule restore esm_gcc_env\n\n# navigate to the ctsm tools\ncd $CTSMROOT/tools/mksurfdata_esmf\n\n# run the script to build the executable\n./gen_mksurfdata_build --machine hipergator\n</code></pre> This should give the following output.</p> <pre><code>Successfully created mksurfdata_esmf executable for: hipergator_gnu for openmpi library\n</code></pre> <p>It is a good idea to check that the <code>parallelio</code> libraries were properly linked. <pre><code># go to the build directory\ncd tool_bld\n\n# examine linked libraries in the executable\nldd mksurfdata | grep libpio\n</code></pre></p> <p>You should see something like <pre><code>libpiof.so =&gt; /parallelio/bld/lib/libpiof.so (0x00001500753d4000)\nlibpioc.so =&gt; /parallelio/bld/lib/libpioc.so (0x0000150075182000)\n</code></pre></p> <p>If it says \"not found\" then the libraries were not linked correctly and you need to retry building the executable.</p> <p>Note: If there is a build error and you need to rerun the script, be sure to delete the <code>tool_build</code> directory before attempting to rebuild.</p>"},{"location":"archive/ctsm-porting/#setup-ctsm_pylib","title":"Setup ctsm_pylib","text":"<p>There is a <code>conda</code> environment that needs to be setup in order to use <code>mksurfdata</code>.</p> <p>If you've never used <code>conda</code> environments, I recommend reading the documentation on HiPerGator's website. This has info on configuration, and building and managing environments. Similar to the <code>.bashrc</code> file there is a <code>.condarc</code> file located in your <code>home</code> directory. I would follow the HiPerGator recommendation to add the following lines to <code>.condarc</code> and change the environment and package install path to somewhere on your <code>/blue</code> storage to avoid crowding your home directory.</p> <pre><code>envs_dirs:\n- /blue/group/user/conda/envs\npkgs_dirs:\n- /blue/group/user/conda/pkgs\n</code></pre> <p>Once you have <code>conda</code> set up to your liking, follow directions in the CTSM tools directory to install the <code>conda</code> environment.</p> <pre><code># Assuming pwd is the tools/mksurfdata_esmf directory\n module load conda\n cd ../..  # or ../../../.. for a CESM checkout)\n ./py_env_create    # Assuming at the top level of the CTSM/CESM checkout\n conda activate ctsm_pylib\n</code></pre> <p>NOTE: WE'RE STILL WORKING ON HOW TO USE THESE TOOLS AS WELL AS RUN SINGLE POINT CASES WITH THIS CTSM VERSION</p>"},{"location":"archive/ctsm-usage/","title":"Ctsm usage","text":""},{"location":"archive/ctsm-usage/#basic-usage-of-ctsm","title":"Basic Usage of CTSM","text":"<p>See the section in <code>CESM</code>. The scripts for general use are located in <code>$CTSMROOT/cime/scripts</code>.</p>"},{"location":"archive/ctsm-usage/#documentation-issues","title":"Documentation Issues","text":"<p>After reading the documentation and the following two comments from CESM forum admins, it looks like everything in the documentation's single point case creation is either outdated or broken.</p> <p>Admin Comments: 1. \"Support for PTS_MODE in cime was dropped at some point so you are likely using a version of the model in which support was dropped.\"    * It is still fine to use on our <code>CESM</code> install.</p> <ol> <li>\"If you want to subset existing global datasets to regional or single point please see the README in <code>tools/site_and_regional</code>. If you want to create your own regional datasets from scratch please see the <code>README</code> in <code>tools/mksurfdata_esmf</code>.\"</li> </ol>"},{"location":"archive/ctsm-usage/#single-point-subset","title":"Single Point Subset","text":"<p>module load conda</p> <p>conda activate ctsm_pylib</p> <p>cd $CTSM/tools/site_and_regional</p> <p>./subset_data point --lat $my_lat --lon $my_lon --site $my_site_name --create-surface --create-datm \\ --datm-syr $my_start_year --datm-eyr $my_end_year --create-user-mods --outdir $my_output_dir</p> <p>$my_lat : latitude of point, should be between -90 and 90 degrees $my_lon : longitude of point, should be between 0 and 360 or -180 and 180 degrees $my_site_name : name of site, used for file naming $my_start_year : start year for DATM data to subset, default between 1901 and 2014 $my_end_year : end year for DATM data to subset, default between 1901 and 2014 $my_output_dir : output directory to place the subset data and user_mods directory</p> <pre><code># cd to subset data tool and run script\ncd $CTSM/tools/site_and_regional\n\n./subset_data point --lat 42.53562 --lon 287.82438 --site demo_pt --create-surface --create-datm --datm-syr 1901 --datm-eyr 1902 --create-user-mods --outdir /blue/gerber/cdevaneprugh/my_subset_data/demo_pt --overwrite\n\n# cd to cime scripts and create new case from dataset\ncd $CTSMROOT/cime/scripts\n\n./create_newcase --case $CASES/singlept.demo --res CLM_USRDAT --compset I1850Clm50Bgc --run-unsupported --user-mods-dirs /blue/gerber/cdevaneprugh/my_subset_data/demo_pt/user_mods/\n\n# cd into case directory, change variables, and run case\ncd /blue/gerber/cdevaneprugh/cases/singlept.demo\n\n./case.setup\n\n./xmlchange MPILIB=openmpi\n./xmlchange STOP_OPTION=nyears\n./xmlchange STOP_N=1\n./xmlchange RUN_STARTDATE='1901-01-01'\n./xmlchange DATM_YR_ALIGN=1901\n./xmlchange DATM_YR_START=1901\n./xmlchange DATM_YR_END=1902\n\n./case.build\n./case.submit\n</code></pre>"},{"location":"archive/forking-ctsm/","title":"CTSM Forking and Submodule Management Guide","text":"<p>This guide provides a step-by-step process for forking the CTSM repository, managing its dependent repositories (submodules), and modifying the repository structure for development.</p>"},{"location":"archive/forking-ctsm/#1-forking-the-ctsm-repository","title":"1. Forking the CTSM Repository","text":"<ol> <li>Navigate to the CTSM repository on GitHub:    https://github.com/ESCOMP/CTSM</li> <li>Click the \"Fork\" button in the upper right to create your copy under your GitHub account.</li> <li>Ensure that all branches are copied, not just the <code>master</code> branch.</li> <li>Clone your fork locally using SSH (recommended for convenience):    <pre><code>git clone git@github.com:your-username/CTSM.git\ncd CTSM\n</code></pre>    If you prefer HTTPS, use:    <pre><code>git clone https://github.com/your-username/CTSM.git\n</code></pre></li> </ol>"},{"location":"archive/forking-ctsm/#2-setting-up-the-upstream-remote","title":"2. Setting Up the Upstream Remote","text":"<p>After cloning your fork, add the original CTSM repository as an upstream remote: <pre><code>git remote add upstream https://github.com/ESCOMP/CTSM.git\n</code></pre> Verify the remotes: <pre><code>git remote -v\n</code></pre> You should see output similar to: <pre><code>origin  git@github.com:your-username/CTSM.git (fetch)\norigin  git@github.com:your-username/CTSM.git (push)\nupstream  https://github.com/ESCOMP/CTSM.git (fetch)\nupstream  https://github.com/ESCOMP/CTSM.git (push)\n</code></pre> - origin \u2192 Your fork (read/write access). - upstream \u2192 The original CTSM repository (read-only access unless you have permissions).</p> <p>To update your fork with the latest changes from upstream: <pre><code>git fetch upstream\ngit merge upstream/main  # Replace 'main' with the correct branch\n</code></pre></p>"},{"location":"archive/forking-ctsm/#3-checking-out-a-specific-branch","title":"3. Checking Out a Specific Branch","text":"<p>To work on the latest CTSM release, switch to the appropriate branch: <pre><code>git checkout ctsm5.3.023\n</code></pre> Create a new branch for modifications: <pre><code>git checkout -b uf-ctsm5.3\n</code></pre></p>"},{"location":"archive/forking-ctsm/#4-managing-git-submodules","title":"4. Managing Git Submodules","text":"<p>CTSM includes submodules (nested repositories), such as <code>ccs_config</code>, which we need for the config files and which must be managed properly.</p>"},{"location":"archive/forking-ctsm/#41-forking-and-updating-submodules","title":"4.1. Forking and Updating Submodules","text":"<p>We need to fork and modify <code>ccs_config</code>: 1. Fork https://github.com/ESMCI/ccs_config_cesm.</p> <ol> <li> <p>Clone your fork locally:    <pre><code>git clone git@github.com:your-username/ccs_config_cesm.git\ncd ccs_config_cesm\n</code></pre></p> </li> <li> <p>Find the tag used in your CTSM checkout. You can find this by looking at the <code>.gitmodules</code> file in <code>$CTSMROOT</code>.</p> </li> </ol> <pre><code>[submodule \"ccs_config\"]\npath = ccs_config\nurl = https://github.com/ESMCI/ccs_config_cesm.git\nfxtag = ccs_config_cesm1.0.20\nfxrequired = ToplevelRequired\n# Standard Fork to compare to with \"git fleximod test\" to ensure personal forks aren't committed\nfxDONOTUSEurl = https://github.com/ESMCI/ccs_config_cesm.git\n</code></pre> <ol> <li>Create a new branch to work on.</li> </ol> <pre><code>git checkout ccs_config_cesm1.0.20\ngit checkout -b uf-config\n</code></pre> <ol> <li> <p>Add the HiPerGator config files to your cloned repo. Commit and push the changes.</p> </li> <li> <p>Create a tag that can be added to <code>.gitmodules</code></p> </li> </ol> <pre><code># create tag and push to github\ngit tag -a uf-config1.0 -m \"Tagging version v1.0 of uf configs\"\ngit push origin uf-config1.0\n</code></pre> <ol> <li>Update the <code>fxtag</code>  line in<code>.gitmodules</code> to point to your forked submodule:</li> </ol> <pre><code>[submodule \"ccs_config\"]\nfxtag = uf-config1.0\n</code></pre>"},{"location":"archive/forking-ctsm/#42-running-git-fleximod-to-update-submodules","title":"4.2. Running <code>git-fleximod</code> to Update Submodules","text":"<p>After modifying submodules, run: <pre><code>./bin/git-fleximod update\n</code></pre> Commit the updated submodule reference: <pre><code>git add ccs_config\ngit commit -m \"Updated submodule reference after git-fleximod update\"\ngit push origin uf-ctsm5.3\n</code></pre></p>"},{"location":"archive/forking-ctsm/#5-creating-and-managing-git-tags","title":"5. Creating and Managing Git Tags","text":"<p>If you need to create a new tag: <pre><code>git tag -a tag-v1.0 -m \"Tagging version v1.0\"\n</code></pre> Push the tag: <pre><code>git push origin tag-v1.0\n</code></pre></p>"},{"location":"archive/forking-ctsm/#overwriting-an-existing-tag","title":"Overwriting an Existing Tag","text":"<p>If you need to update an existing tag: <pre><code>git tag -d tag-v1.0\n</code></pre> Delete the old tag from the remote: <pre><code>git push --delete origin tag-v1.0\n</code></pre> Create a new tag with the same name: <pre><code>git tag -a tag-v1.0 -m \"Updated tag\"\ngit push origin tag-v1.0\n</code></pre></p>"},{"location":"archive/forking-ctsm/#deleting-multiple-local-tags","title":"Deleting Multiple Local Tags","text":"<p>To delete multiple local tags: <pre><code>git tag -d tag1 tag2 tag3\n</code></pre> To delete the same tags remotely: <pre><code>git push --delete origin tag1 tag2 tag3\n</code></pre> To refresh tags from the remote repository: <pre><code>git fetch --prune --tags\n</code></pre></p>"},{"location":"archive/forking-ctsm/#6-summary-of-workflow","title":"6. Summary of Workflow","text":"<ol> <li>Fork CTSM and its submodules, ensuring all branches are copied.</li> <li>Clone your fork using SSH for convenience.</li> <li>Add the original CTSM repository as <code>upstream</code> and fetch updates regularly.</li> <li>Checkout the appropriate branch and create a working branch for modifications.</li> <li>Manage submodules properly by forking them and updating <code>.gitmodules</code>.</li> <li>Run <code>git-fleximod update</code> to update submodules.</li> <li>Create, update, or delete Git tags as needed.</li> </ol>"},{"location":"archive/glossary-old/","title":"Glossary old","text":""},{"location":"archive/glossary-old/#glossary","title":"Glossary","text":"<p>CESM - Community Earth System Model. CESM is a fully-coupled, community maintained, global climate model that provides state-of-the-art computer simulations of the Earth's past, present, and future climate states.</p> <p>CIME - Common Infrastructure for Modeling the Earth (pronounced \u201cSEAM\u201d) provides a Case Control System for configuring, compiling and executing Earth system models, data and stub model components, a driver and associated tools and libraries.</p> <p>CLM - Community Land Model. The land component used in CESM. It has the capability to model specific processes such as vegetation composition, heat transfer in soil, carbon-nitrogen cycling, canopy hydrology, and many more.</p> <p>CLM_USRDAT_NAME</p> <ul> <li>Provides a way to enter your own datasets into the namelist. The files you create must be named with specific naming conventions outlined in Creating your own single-point dataset.</li> </ul> <p>cprnc</p> <ul> <li>Tool used to compare two NetCDF files.</li> <li>Appears to have been previously located at <code>$CIMEROOT/tools/cprnc</code>, now installed in <code>/blue/gerber/earth_models/cprnc</code>. </li> </ul> <p>CTSM - Community Terrestrial Systems Model. An extension of CLM that is meant to be run as an alternative to CESM.</p> <p>E3SM - Energy Exascale Earth System Model. E3SM is the Department of Energy's climate model. Forked from CESM v1 and developed independently by the DOE, they describe E3SM as \"an ongoing, state-of-the-science Earth system modeling, simulation, and prediction project that optimizes the use of DOE laboratory resources to meet the science needs of the nation and the mission needs of DOE.\"</p> <p>HPG - HiPerGator. The supercomputer used at UF. I'll use HPG and HiPerGator interchangeably throughout the documentation.</p> <p>GGCMI Global Gridded Crop Model Inter comparisons</p> <ul> <li>Group of crop modelers who provide gridded crop and climate data.</li> <li>Relevant to the scripts in <code>$CTSMROOT/tools/crop_calendars</code></li> </ul> <p>Namelist (in the context of <code>mksurfdata</code>)</p> <ul> <li>Looks like different variables/parameters for the model.</li> </ul> <p>nuopc The National Unified Operational Prediction Capability</p> <ul> <li>A consortium of Navy, NOAA, and Air Force modelers and their research partners. It aims to advance the weather prediction modeling systems used by meteorologists, mission planners, and decision makers. </li> <li>NUOPC partners are working toward a standard way of building models in order to make it easier to collaboratively build modeling systems. To this end, they have developed the NUOPC Layer that defines conventions and a set of generic components for building coupled models using the Earth System Modeling Framework (ESMF).</li> </ul> <p>PTS_MODE</p> <ul> <li>A deprecated way to run single point simulations (still works on cesm 2.1.5).</li> <li>Okay for quick and dirty runs.</li> <li>Lacks restart capability.</li> </ul> <p>SCRIP Spherical Coordinate Remapping and Interpolation Package</p> <ul> <li>Computes addresses and weights for remapping and interpolating fields between grids in spherical coordinates.</li> <li>Looks like they are used to create the surface datasets.</li> </ul>"},{"location":"archive/intro-to-earth-models/","title":"An Overview of Earth System Models","text":""},{"location":"archive/intro-to-earth-models/#community-earth-system-model-cesm","title":"Community Earth System Model (CESM)","text":"<p>Created by: The National Center for Atmospheric Research (NCAR) with contributions from the scientific community.</p>"},{"location":"archive/intro-to-earth-models/#history","title":"History","text":"<ul> <li>Developed from the Community Climate System Model (CCSM) in the 1990s.</li> <li>CESM v1.0 was released in 2010 as an improved coupled climate model.</li> <li>Current versions, including CESM2.x, support high-resolution simulations and enhanced climate interactions.</li> </ul>"},{"location":"archive/intro-to-earth-models/#technical-overview","title":"Technical Overview","text":"<ul> <li>CESM is a fully coupled, global climate model that simulates the interactions of the Earth\u2019s atmosphere, land, ocean, and ice systems.</li> <li>Built on Common Infrastructure for Modeling the Earth (CIME), providing tools for configuring, building, and executing model cases.</li> <li>Utilizes a component-based architecture with interchangeable modules for different Earth system components (atmosphere, land, ocean, etc.).</li> <li>Supports various grid resolutions and scientific use cases, such as paleoclimate studies, future climate projections, and single-point land simulations.</li> </ul>"},{"location":"archive/intro-to-earth-models/#community-terrestrial-systems-model-ctsm","title":"Community Terrestrial Systems Model (CTSM)","text":"<p>Created by: NCAR, evolving from the Community Land Model (CLM).</p>"},{"location":"archive/intro-to-earth-models/#history_1","title":"History","text":"<ul> <li>Initially, land modeling within CESM was handled by CLM, a component developed since the early 2000s.</li> <li>CTSM was introduced as an enhanced land model that can operate independently or within CESM.</li> <li>CTSM5 is the latest version, widely used in land-atmosphere interactions and biogeochemical modeling.</li> </ul>"},{"location":"archive/intro-to-earth-models/#technical-overview_1","title":"Technical Overview","text":"<ul> <li>CTSM extends CLM and serves as an alternative land model within CESM or as a standalone model.</li> <li>Simulates vegetation dynamics, carbon-nitrogen cycling, and land hydrology processes.</li> <li>Employs CIME infrastructure for model configuration and case management.</li> <li>Can be used for global, regional, or single-point simulations, depending on research needs.</li> <li>Requires custom-built surface datasets and mapping files to translate model grids into usable land surface information.</li> </ul>"},{"location":"archive/intro-to-earth-models/#energy-exascale-earth-system-model-e3sm","title":"Energy Exascale Earth System Model (E3SM)","text":"<p>Created by: The U.S. Department of Energy (DOE).</p>"},{"location":"archive/intro-to-earth-models/#history_2","title":"History","text":"<ul> <li>Developed as a fork of CESM v1 by DOE laboratories, including LLNL, ANL, ORNL, and PNNL.</li> <li>Launched as an independent model focused on high-performance computing (HPC) and energy-related climate impacts.</li> <li>Optimized for DOE\u2019s exascale computing resources.</li> </ul>"},{"location":"archive/intro-to-earth-models/#technical-overview_2","title":"Technical Overview","text":"<ul> <li>Similar to CESM, E3SM is a fully coupled Earth system model, but optimized for higher resolution and DOE-specific climate objectives.</li> <li>Features enhanced land-atmosphere coupling and regional refinement capabilities.</li> <li>Utilizes CIME infrastructure but diverges in model physics and component designs.</li> <li>Supports targeted simulations for climate change impacts on energy production, water availability, and extreme weather events.</li> </ul>"},{"location":"archive/intro-to-earth-models/#how-coupled-climate-and-earth-models-work","title":"How Coupled Climate and Earth Models Work","text":"<p>Coupled climate and Earth system models integrate multiple interacting components to simulate complex planetary systems over time. These models link the atmosphere, ocean, land, and ice to understand climate dynamics and make future predictions.</p>"},{"location":"archive/intro-to-earth-models/#key-components","title":"Key Components","text":"<ul> <li>Atmosphere: Simulates temperature, wind, humidity, and radiation through atmospheric circulation models.</li> <li>Ocean: Models currents, salinity, and temperature variations to study oceanic interactions with climate.</li> <li>Land: Includes vegetation, soil moisture, and carbon cycle processes to track terrestrial climate effects.</li> <li>Ice: Simulates ice sheets and sea ice to examine polar climate changes.</li> </ul>"},{"location":"archive/intro-to-earth-models/#process-coupling","title":"Process Coupling","text":"<ul> <li>Advanced numerical methods solve fluid dynamics and thermodynamic equations.</li> <li>Exchanges between different components occur through fluxes of heat, moisture, and momentum.</li> <li>High-performance computing (HPC) enables large-scale simulations with fine spatial and temporal resolutions.</li> </ul>"},{"location":"archive/intro-to-earth-models/#applications","title":"Applications","text":"<ul> <li>Predicting future climate scenarios based on different greenhouse gas emission pathways.</li> <li>Studying extreme weather events, ocean circulation shifts, and ecosystem responses to climate change.</li> <li>Supporting decision-making in climate policy, energy planning, and disaster management.</li> </ul> <p>For further details, refer to:</p> <ul> <li>DOE Explanation of Earth System Models</li> <li>Nature Climate Modeling Overview</li> </ul>"},{"location":"archive/intro-to-earth-models/#shared-technical-aspects","title":"Shared Technical Aspects","text":"<ul> <li>CIME (Common Infrastructure for Modeling the Earth)</li> <li>A framework used by all three models for configuring, compiling, and running simulations.</li> <li>Provides machine-specific configurations, job submission tools, and workflow management.</li> <li>Used to adapt the models for various HPC environments, such as HiPerGator.</li> </ul>"},{"location":"archive/linux-overview/","title":"Linux overview","text":"<p>If you are new to HiPerGator or Linux I suggest you read through this document and utilize at least one of the resources linked below. While HiPerGator offers access to the system with a GUI or the ability to code via Jupyter notebooks, the Earth models will require you to use the command line exclusively, so it's important you are comfortable doing so.</p>"},{"location":"archive/linux-overview/#contents","title":"Contents","text":"<ol> <li>General Linux Information</li> <li>Basic Commands</li> <li>Text Editors</li> <li>External Resources</li> <li>HiPerGator Specific Information</li> <li>lmod</li> <li>SLURM</li> <li>Useful Commands</li> <li>.bashrc File</li> </ol>"},{"location":"archive/linux-overview/#1-general-linux-information","title":"1. General Linux Information","text":"<p>The Linux command line, also referred to as the terminal or shell, is a text-based interface for interacting with the Linux operating system. It allows users to execute commands and perform various tasks efficiently, without relying on a graphical user interface.</p> <p>Command Line Advantages:</p> <ul> <li>Efficiency: The command line provides quick access to powerful tools and utilities, enabling you to perform tasks more efficiently compared to a GUI.</li> <li>Automation: Command line scripting allows for the automation of repetitive tasks, saving time and effort.</li> <li>Flexibility: Advanced users can customize and tailor their workflow to suit their specific needs, leveraging the extensive capabilities of the command line.</li> </ul> <p>Bash: Bash (Bourne Again Shell) is one of the most commonly used command line interpreters in Linux. It is the default shell for most Linux distributions (including HiPerGator) and provides a powerful scripting environment for automation and system administration tasks.</p>"},{"location":"archive/linux-overview/#11-basic-commands","title":"1.1 Basic Commands:","text":"<ol> <li>pwd (Print Working Directory):</li> <li>Displays the current directory path.</li> <li>Example: If you are in the \"home\" directory of the user \"cooper,\" <code>pwd</code> would output <code>/home/cooper</code> in your terminal.</li> <li>ls (List):</li> <li>Lists files and directories in the specified directory (or current directory if one isn't given).</li> <li>Example: <code>ls my_files</code> would list the contents of the directory \"my_files.\"</li> <li>cd (Change Directory):</li> <li>Changes the current directory to the specified one.</li> <li>Example: <code>cd dir1</code> will change your current working directory to <code>dir1</code> so long as that directory is in your current working path.</li> <li>If no directory is specified, <code>cd</code> will move you to your <code>home</code> directory.</li> <li>touch:</li> <li>Creates an empty file.</li> <li>Example: <code>touch filename.txt</code> creates an empty text file.</li> <li>mkdir (Make Directory):</li> <li>Similar to <code>touch</code>, but creates an empty directory instead. </li> <li>As with all these commands, you can use relative or absolute paths. For example <code>mkdir dir1</code> creates an empty directory named \"dir1\" in the current directory. <code>mkdir /home/cooper/dir1</code> creates \"dir1\" in Cooper's home directory.</li> <li>rm (Remove):</li> <li>Delete files or directories.</li> <li>Example: <code>rm filename.txt</code> (for files), <code>rm -r directory</code> (for directories where the <code>-r</code> flag specifies to remove things recursively).</li> <li>cp (Copy):</li> <li>Copies files or directories.</li> <li>Example: <code>cp file1 file2</code> (to copy file1 to file2), <code>cp -r dir1 dir2</code> (to copy dir1 to dir2 recursively).</li> <li>mv (Move):</li> <li>Moves or renames files or directories.</li> <li>Example: <code>mv file1 file2</code> (to rename file1 to file2), <code>mv file1 /path/to/directory</code> (to move file1 to another directory).</li> <li>grep (Global Regular Expression Print):</li> <li>Searches for text patterns.</li> <li>Example: <code>grep foo notes.txt</code> searches for the term \"foo\" in the file notes.txt.</li> <li>chmod</li> <li>Access and change permissions.</li> <li>Example: <code>chmod +x MY_SCRIPT.sh</code> would add the ability for a script you have written to be executed.</li> <li>man (Manual):<ul> <li>Displays manual pages for commands, providing detailed information about their usage and options.</li> <li>Example: <code>man ls</code> displays the manual page for the <code>ls</code> command.</li> </ul> </li> <li>Piping (|):<ul> <li>Allows the output of one command to be used as input to another command.</li> <li>Example: <code>command1 | command2</code> (output of <code>command1</code> is used as input for <code>command2</code>).</li> <li>Example: <code>ls Documents | grep my_file</code> Will search the \"Documents\" directory for files titled \"my_file.\" You could also add the <code>-r</code> flag to the <code>grep</code> command to search within each file for the search term.</li> </ul> </li> </ol>"},{"location":"archive/linux-overview/#12-text-editors","title":"1.2 Text Editors","text":"<p>The three main text editors used on Linux are Vim, Emacs, and Nano. <code>nano</code> is the most basic, and easiest to use. <code>vim</code> and <code>emacs</code> are extremely customizable, but have a substantially steeper learning curve. Between <code>vim</code> and <code>emacs</code>, <code>vim</code> is arguably the most difficult to get comfortable with, but is powerful once you do. If you decide you are brave enough to try it, sites like openvim or running the command <code>vimtutor</code> (which starts an interactive tutorial) on any Linux machine are good places to start. Additionally here is a cheat sheet of <code>vim</code> commands that serve as a nice reference.</p> <p>WARNING The default text editor on Linux is usually <code>vim</code>. There may be a time when you accidentally open a file in <code>vim</code> and can't figure out how to exit the program. This may sound silly, but I promise you, the jokes about <code>vim</code> being confusing are endless in the Linux community. If you do find yourself trapped in <code>vim</code> hit the <code>esc</code> key, then type <code>:q!</code> and hit <code>enter</code>. This will exit whatever file you got yourself into without saving any changes. If you would like to make a quick change to the file you got into, hit <code>esc</code>, followed by the <code>i</code> key to enter \"insert\" mode. At this point you can type whatever you need (use the arrow keys for navigation). To save, use <code>esc</code> to exit insert mode, then type <code>:wq</code> and hit <code>enter</code>. This will write the file (<code>w</code>) then quit out of the editor (<code>q</code>). </p>"},{"location":"archive/linux-overview/#13-external-resources","title":"1.3 External Resources","text":"<p>Here are some resources to get started using the Linux command line, and learn a bit more about what is going on \"under the hood.\" You don't need to memorize everything in these links, but they can serve as a good starting point.</p> <ol> <li>https://linuxjourney.com/</li> </ol> <p>The \"Grasshopper\" and \"Journeyman\" sections will give you a good chunk of background knowledge. I should note that because HiPerGator is not a traditional computer, some of the information (like package management) will not directly translate. However, it will help build a foundation of Linux knowledge and is certainly worth being exposed to. </p> <ol> <li>HiPerGator's Introduction to the Linux Command Line</li> </ol> <p>A good demonstration of basic Linux commands you will be using on a daily basis, along with some practice exercises. I highly recommend going through this entire lesson and watching the accompanying training video.</p> <ol> <li>jlevy's art of the command line</li> </ol> <p>This write up covers a wide range of commands and could serve as a good reference sheet. </p> <ol> <li> <p>YouTube has plenty of tutorials. Here's a good one that serves as an introduction to the command line.</p> </li> <li> <p>ChatGPT is actually pretty good at <code>bash</code> scripting or serving as an interactive assistant.</p> </li> </ol>"},{"location":"archive/linux-overview/#2-hipergator-specific-information","title":"2. HiPerGator Specific Information","text":"<p>HiPerGator is not set up like a traditional personal computer. While Google and sites like stack overflow are great resources to use when you run into problems, remember to check HiPerGator's documentation first, as it will have information specific to our system.</p> <p>Additionally, when using Linux, you should build the habit of using the commands <code>man</code> and the flag <code>--help</code> to get yourself out of trouble. <code>man</code> will access the built in documentation for Linux commands and programs that are on the system. For example, say you want to learn about the <code>ls</code> command, and all the options it has. You can type <code>man ls</code> to pull up the documentation page. You can also type <code>ls --help</code> for a list available arguments and options. </p> <p>If you are new to HiPerGator I would highly recommend reading through their FAQ,  Getting Started page, and watching all of the available training videos provided in the documentation.</p>"},{"location":"archive/linux-overview/#21-lmod","title":"2.1 lmod","text":"<p>HiPerGator uses a program called <code>lmod</code> to be able to have multiple versions of the same software installed for different use cases. Here is a good article that gives an overview of module systems and how they work. While the HiPerGator documentation has great info that I suggest you read, the full documentation for <code>lmod</code> can be found here.</p>"},{"location":"archive/linux-overview/#22-slurm","title":"2.2 SLURM","text":"<p>Unlike a personal computer, HiPerGator is a collection of connected computers (called nodes) that share resources. The node that you login to (unsurprisingly called a login node) has a limited amount of resources, and is generally used for tasks like writing code, file management, and very light jobs. As such, you will submit more computationally \"expensive\" jobs to the <code>SLURM</code> scheduler to run on the dedicated compute nodes.</p> <p>Please read the page on our scheduler here. The full documentation for the <code>SLURM</code> scheduler can be found here.</p>"},{"location":"archive/linux-overview/#23-useful-commands","title":"2.3 Useful Commands","text":"<p>Here are some miscellaneous useful commands for navigating and managing your environment in HiPerGator:</p> <ol> <li> <p><code>env | grep HPC | sort</code></p> </li> <li> <p>Lists all environment variables related to HiPerGator and sorts them alphabetically.</p> </li> <li> <p>Useful for checking loaded modules, system paths, and predefined environment settings.</p> </li> <li> <p><code>tree | less</code></p> </li> <li> <p>The <code>tree</code> command displays a directory structure in a hierarchical format, which is helpful for visualizing project layouts.</p> </li> <li> <p>The <code>less</code> command allows you to scroll through large outputs without flooding your terminal.</p> </li> <li> <p>SLURM Resource Monitoring and Job Management</p> </li> <li> <p><code>squeue -u $USER</code>: This lists all jobs associated with your username, showing job IDs, status, and resources requested. Can also use <code>$GROUP</code> to see all current jobs associated with the group.</p> </li> <li> <p><code>scontrol show job JOB_ID</code>: Replace <code>JOB_ID</code> with the actual job ID from <code>squeue</code> to see job details.</p> </li> <li><code>scancel JOB_ID</code>: This will stop a running job. Replace JOB_ID with <code>$USER</code> to stop all user jobs.</li> </ol>"},{"location":"archive/linux-overview/#24-bashrc-file","title":"2.4 .bashrc File","text":"<p>The <code>.bashrc</code> file is a configuration script that runs automatically whenever you open a new shell session. It is located in your home directory (<code>~/.bashrc</code>) and is used to set environment variables, define aliases, and customize your shell environment. Here are a few things that are helpful to add to your <code>.bashrc</code> file, but not required.</p>"},{"location":"archive/linux-overview/#adding-environment-variables","title":"Adding Environment Variables","text":"<p>Environment variables store system-wide or user-specific configurations. You can add variables to <code>.bashrc</code> like this:</p> <pre><code>export MY_ENV_VAR=\"my_value\"\n</code></pre> <p>To define your Earth System Models (ESM) directory, group name, and case output directories add the following to the end of your <code>.bashrc</code> file:</p> <pre><code>export ESM_DIR=\"/blue/gerber/earth_models\"\nexport GROUP=\"gerber\"\nexport CASE_OUTPUT_DIR=\"/blue/$GROUP/$USER/cases\"\n</code></pre>"},{"location":"archive/linux-overview/#reloading-bashrc","title":"Reloading <code>.bashrc</code>","text":"<p>Whenever you modify your <code>.bashrc</code> file, you need to reload it for the changes to take effect. To apply the changes without logging out:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>Alternatively, you can log out and log back in to ensure the new environment variables are loaded.</p>"},{"location":"archive/mksurfdata-fixes/","title":"CTSM File Modifications","text":"<p>We need to modify several buggy files in CTSM in order to build <code>mksurfdata</code>. I have forked CTSM and fixed these bugs in the uf repo. If that is not available for some reason, here is how to fix the existing code. </p> <p>First go to the directory at <code>$CTSMROOT/tools/mksurfdata_esmf</code>.</p>"},{"location":"archive/mksurfdata-fixes/#cmakeliststxt","title":"CMakeLists.txt","text":"<p>Then modify the <code>CMakeLists.txt</code> file at <code>CTSMROOT/tools/mksurfdata_esmf/src/CMakeLists.txt</code>. <code>CMakeLists.txt</code> is a configuration file that CMake uses to define the build process, including source files, dependencies, build targets, and project settings.</p> <p>At lines 51 and 52, change the word STATIC to SHARED so the lines read as follows.</p> <pre><code>add_library(pioc SHARED IMPORTED)\nadd_library(piof SHARED IMPORTED)\nset_property(TARGET pioc PROPERTY IMPORTED_LOCATION $ENV{PIO}/lib/libpioc.so)\nset_property(TARGET piof PROPERTY IMPORTED_LOCATION $ENV{PIO}/lib/libpiof.so)\n</code></pre> <p>If you notice in the bottom two lines, we are looking for the shared libraries <code>libpioc.so</code> and <code>libpiof.so</code>. This conflicted with the previous two lines looking for static libraries.</p>"},{"location":"archive/mksurfdata-fixes/#mksurfdataf90","title":"mksurfdata.F90","text":"<p>Next we have to correct some Fortran syntax errors in <code>src/mksurfdata.F90</code>. If you try to compile you will be met with the following errors.</p> <pre><code>/ctsm5.3/tools/mksurfdata_esmf/src/mksurfdata.F90:271:24:\n\n  271 |      write(ndiag,'(2(a,I))') ' npes = ', npes, ' grid size = ', grid_size\n      |                        1\nError: Nonnegative width required in format string at (1)\n\n/ctsm5.3/tools/mksurfdata_esmf/src/mksurfdata.F90:295:19:\n\n  295 |      read(nfpio, '(i)', iostat=ier) pio_iotype\n      |                   1\nError: Nonnegative width required in format string at (1)\n\n/ctsm5.3/tools/mksurfdata_esmf/src/mksurfdata.F90:328:27:\n\n  328 |         write (ndiag,'(a, I, a, I)') ' node_count = ', node_count, ' grid_size = ', grid_size\n      |                           1\nError: Nonnegative width required in format string at (1)\n</code></pre> <p>To fix this we make the following changes at lines 271, 295, and 328 respectively.</p> <pre><code>271 |      write(ndiag,'(2(a,I10))') ' npes = ', npes, ' grid size = ', grid_size\n\n295 |      read(nfpio, '(I5)', iostat=ier) pio_iotype\n\n328 |         write (ndiag,'(a, I10, a, I10)') ' node_count = ', node_count, ' grid_size = ', grid_size\n</code></pre>"},{"location":"archive/mksurfdata-fixes/#build-script","title":"Build Script","text":"<p>Finally, modify line 170 of the <code>gen_mksurfdata_build</code> script to the following.</p> <pre><code>CC=mpicc FC=mpif90 cmake -DCMAKE_BUILD_TYPE=Debug -DCMAKE_Fortran_FLAGS=\" -fallow-argument-mismatch -fallow-invalid-boz -ffree-line-length-none\" $options $cwd/src\n</code></pre>"},{"location":"archive/shared-utils/","title":"Shared Utilities and Setup Guide","text":""},{"location":"archive/shared-utils/#1-general-directory-structure","title":"1. General Directory Structure","text":"<p>The Gerber group maintains a shared directory on HiPerGator for Earth system models at:</p> <pre><code>/blue/gerber/earth_models\n</code></pre> <p>Note that there are many ways your research group could decide to set this up, this is just what we have chosen to do.</p> <p>This directory contains:</p> <ul> <li>Earth model source code (e.g., CESM, CTSM)</li> <li>Input data (boundary conditions, forcing datasets)</li> <li>Shared utilities (e.g., <code>cprnc</code>, <code>parallelio</code>, custom scripts)</li> </ul> <pre><code>earth_models/\n\u251c\u2500\u2500 cesm2.1.5           # cesm earth model root directory\n\u251c\u2500\u2500 config              # config files for each model as well as utility scripts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cesm2.1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ctsm5.3\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 hpg\n\u251c\u2500\u2500 ctsm5.3             # ctsm earth model root directory\n\u251c\u2500\u2500 docs                # documentation (you are here)\n\u251c\u2500\u2500 inputdata           # input data shared between models\n\u251c\u2500\u2500 README\n\u251c\u2500\u2500 scripts             # misc. scripts\n\u2514\u2500\u2500 shared              # shared libraries and utilities\n    \u251c\u2500\u2500 cprnc\n    \u2514\u2500\u2500 parallelio\n</code></pre>"},{"location":"archive/shared-utils/#2-required-modules-and-environment-setup","title":"2. Required Modules and Environment Setup","text":""},{"location":"archive/shared-utils/#dedicated-module-environment","title":"Dedicated Module Environment","text":"<p>For consistency, we define a dedicated module collection for Earth system modeling. This setup ensures the correct libraries are loaded every time.</p> <p>There is a script in this repo located at <code>scripts/module.env.setup</code> that will set the environment up with the name \"ctsm-modules\".</p> <p>The steps to do it manually are: <pre><code>module purge  # Remove conflicting modules\n\n# Load essential dependencies\nmodule load subversion/1.9.7 \nmodule load perl/5.24.1 \nmodule load cmake/3.26.4\nmodule load python/3.12\nmodule load gcc/14.2.0\nmodule load lapack/3.11.0\nmodule load openmpi/5.0.7\nmodule load netcdf-c/4.9.3 netcdf-f/4.6.2\nmodule load hdf5\nmodule load esmf/8.8.1\n\n# Save the collection\nmodule save MY_MODULE_COLLECTION\n\n# Verify correct setup\nmodule purge\nmodule restore MY_MODULE_COLLECTION\nmodule list\n</code></pre></p> <p>Using this saved environment minimizes issues when running scripts and compiling models. </p>"},{"location":"archive/shared-utils/#3-configuring-cime-on-hipergator","title":"3. Configuring CIME on HiPerGator","text":"<p>CIME (Common Infrastructure for Modeling the Earth) manages the configuration, build, and execution of Earth models. There are three critical configuration files: one for compilers, one for the batch system, and one for the computing environment.</p> <p>There are two ways to configure these files for CIME:</p> <ol> <li> <p>Modify the three files located in <code>$CIMEROOT/config/$model/machines/</code> by adding entries for HiPerGator.</p> </li> <li> <p>Alternatively, create standalone personal config files in </p> </li> </ol> <pre><code>~/.cime/\n</code></pre> <p>These append onto the default files when the model runs.</p> <pre><code>~/.cime/\n\u251c\u2500\u2500 config_batch.xml\n\u251c\u2500\u2500 config_compilers.xml\n\u2514\u2500\u2500 config_machines.xml\n</code></pre> <p>If you plan on installing only one Earth model, option two is recommended. Basic HiPerGator configuration files can be found in the <code>config</code> directory of this repository. Due to subtle variations in config files between the Earth models and releases, these may require some modification. The Gerber group, uses multiple models, each using different CIME versions. Therefore, we have opted to fork the CTSM repository and incorporate configuration files there to simplify installation. Users only need to update relevant paths to match their research group and personal directories if they wish to run CTSM. It's possible we will fork CESM in the future as well.</p>"},{"location":"archive/shared-utils/#example","title":"Example","text":"<p>Say you installed <code>cesm</code> in a directory named <code>my-cesm-install</code>. Here are the two possible methods of setting up the configuration files.</p> <p>Method 1: Appending Default files with HiPerGator Settings</p> <p>Using your text editor of choice, copy the contents of <code>config_machines.xml</code>, <code>config_batch.xml</code>, and <code>config_compilers.xml</code> (or <code>gnu_hipergator.cmake</code> depending on which version of cesm you are porting) and add them to the end of their matching files located in <code>my-cesm-install/cime/config/cesm/machines</code>. The <code>config_compilers.xml</code> file should generally be left alone. <code>config_batch.xml</code> will need to be edited to match your group's QoS limits. <code>config_machines.xml</code> will need to be edited to match your group's directory structure.</p> <p>Method 2: Add .cime to Your Home Directory</p> <pre><code># create your .cime directory\nmkdir ~/.cime\n\n# from the root of this directory copy the config files that correspond to the model you are porting\ncp -r config/cesm ~/.cime\n</code></pre> <p>Once again, make sure to modify the configs to match your group's QoS and directory structure.</p>"},{"location":"archive/shared-utils/#4-installing-shared-libraries","title":"4. Installing Shared Libraries","text":""},{"location":"archive/shared-utils/#41-for-the-gerber-group","title":"4.1 For The Gerber Group","text":"<p>The utilities in this section have already been built. Sections 4.2 and 4.3 are meant to provide a record of how they were installed, as well as give guidance to other HiPerGator research groups.</p>"},{"location":"archive/shared-utils/#42-installing-the-cprnc-tool","title":"4.2 Installing the <code>cprnc</code> Tool","text":"<p><code>cprnc</code> compares NetCDF output files. While not required for running Earth models, it is recommended.</p>"},{"location":"archive/shared-utils/#installation-steps","title":"Installation Steps","text":"<p>Ensure required modules are loaded:</p> <pre><code>module load gcc/14.2.0 openmpi/5.0.7 netcdf-c/4.9.3 netcdf-f/4.6.2\n</code></pre> <p>Clone and build:</p> <pre><code>git clone https://github.com/ESMCI/cprnc.git cprnc\n\ncd cprnc\n\nmkdir bld\ncd bld\ncmake ../\nmake\n</code></pre> <p>The executable will be located at:</p> <pre><code>cprnc/bld/cprnc\n</code></pre>"},{"location":"archive/shared-utils/#43-building-parallel-io-pio","title":"4.3 Building Parallel I/O (PIO)","text":"<p>If you plan on making your own surface datasets via the <code>mksurfdata</code> utility, this library is required. The location you install it in is up to you. We opted to clone the repo and build in  <code>earth_models/shared/parallelio</code> , as we have multiple Earth models running in parallel that may need this library. If you are only running one model, you could install it in the default directory at <code>$CTSMROOT/libraries/parallelio</code>.</p> <p>There is also a script provided to do this located at <code>/scripts/build.pio</code></p> <pre><code># restore defalut modules\nmodule restore MY_MODULE_COLLECTION\n\n# cd to library location\ncd $CTSMROOT/libraries/parallelio\nmkdir bld\n\ncmake \\\n  -DNetCDF_C_PATH=/apps/gcc/12.2.0/openmpi/5.0.7/netcdf-c/4.9.3 \\\n  -DNetCDF_Fortran_PATH=/apps/gcc/12.2.0/openmpi/5.0.7/netcdf-f/4.6.2 \\\n  -DWITH_PNETCDF=OFF \\\n  -DBUILD_SHARED_LIBS=ON \\\n  -DCMAKE_INSTALL_PREFIX=`pwd`/bld \\\n  -DCMAKE_INSTALL_RPATH=`pwd`/src/gptl \\\n  -DCMAKE_INSTALL_RPATH_USE_LINK_PATH=TRUE \\\n  ../\n\nmake\nmake install\n</code></pre> <p>Verify successful installation:</p> <pre><code>ls -l bld/lib\n</code></pre>"},{"location":"archive/troubleshooting-old/","title":"Troubleshooting","text":""},{"location":"archive/troubleshooting-old/#types-of-failures","title":"Types of Failures","text":""},{"location":"archive/troubleshooting-old/#build-failures","title":"Build Failures","text":""},{"location":"archive/troubleshooting-old/#runtime-failures","title":"Runtime Failures","text":""},{"location":"archive/troubleshooting-old/#common-slurm-job-failure-messages","title":"Common SLURM Job Failure Messages","text":"<p>When running jobs on an HPC cluster using SLURM, users may encounter failure messages indicating why a job did not complete successfully. Below are common SLURM error messages and their typical causes and solutions.</p> <ol> <li>NODE_FAIL</li> <li>Message: <code>Job &lt;job_id&gt; failed, node failure</code></li> <li>Cause: The compute node running the job encountered a hardware or system failure.</li> <li>Solution:</li> <li>Check system logs (<code>/var/log/messages</code>) or consult the cluster administrators.</li> <li>Resubmit the job.</li> <li> <p>Use <code>sinfo</code> to check the node status and avoid requesting faulty nodes.</p> </li> <li> <p>TIMEOUT</p> </li> <li>Message: <code>Job &lt;job_id&gt; cancelled at &lt;timestamp&gt; because it expired</code></li> <li>Cause: The job exceeded its allocated wall time (<code>--time</code> limit).</li> <li>Solution:</li> <li>Request more time with <code>--time=HH:MM:SS</code>.</li> <li>Optimize the job to complete within the time limit.</li> <li> <p>Use checkpointing to save progress and restart the job if needed.</p> </li> <li> <p>OUT_OF_MEMORY</p> </li> <li>Message: <code>Job &lt;job_id&gt; killed due to out-of-memory (OOM) condition</code></li> <li>Cause: The job used more memory than allocated (<code>--mem</code> or <code>--mem-per-cpu</code>).</li> <li>Solution:</li> <li>Increase memory allocation using <code>--mem=XXGB</code> or <code>--mem-per-cpu=XXMB</code>.</li> <li>Profile memory usage using <code>sstat -j &lt;job_id&gt; --format=MaxRSS</code>.</li> <li> <p>Optimize the program to use memory more efficiently.</p> </li> <li> <p>FAILED</p> </li> <li>Message: <code>Job &lt;job_id&gt; failed with exit code &lt;X&gt;</code></li> <li>Cause: The program crashed due to an error such as segmentation fault or missing dependencies.</li> <li>Solution:</li> <li>Check the job output file (<code>slurm-&lt;job_id&gt;.out</code>) for error messages.</li> <li>Debug using tools like <code>gdb</code>, <code>strace</code>, or <code>valgrind</code>.</li> <li> <p>Ensure all required modules and libraries are loaded before job execution.</p> </li> <li> <p>DEPENDENCY NEVER SATISFIED</p> </li> <li>Message: <code>Job &lt;job_id&gt; dependency never satisfied</code></li> <li>Cause: A dependent job (<code>--dependency=afterok:&lt;job_id&gt;</code>) failed or was never completed.</li> <li>Solution:</li> <li>Check the status of the parent job using <code>sacct -j &lt;job_id&gt;</code>.</li> <li>Resolve any failures in the dependency chain before resubmitting.</li> <li>Consider removing dependencies if they are not necessary.</li> </ol>"},{"location":"archive/troubleshooting-old/#log-files","title":"Log Files","text":"<pre><code># cd to case directory\ncd /case/directory/log/files\n\n# list logs by time modified\nls -t\n\n# inspect the most recently modified log file (most likely where failure is indicated)\ntail logfile\n</code></pre> <p>Another way is to grep for errors recursively through your case files. <pre><code># -i flag to ignore case\n# -r flag to search recursively\ngrep -ir error\n</code></pre></p>"},{"location":"archive/troubleshooting-old/#the-casestatus-file","title":"The CaseStatus File","text":""},{"location":"archive/troubleshooting-old/#did-my-case-fail-or-time-out","title":"Did My Case Fail, or Time Out?","text":"<p>There may be a situation that arises where it is difficult to tell if a case has failed, or just timed out. Here's one way to check if it is a time out issue.</p> <pre><code># cd to your case's run directory\ncd /blue/GROUP/USER/earth_model_output/cime_output_root/CASE/run\n\n# save the names of the run time log files to a `bash` variable\nLOGS=$(ls | grep .log)\n\n# use the stat command to see when they all were last modified\nstat $LOGS | grep Modify\n</code></pre> <p>If all the times printed to the terminal are within a few seconds to a few minutes of each other, your case likely timed out. You can try rebuilding the case after increasing the <code>JOB_WALLCLOCK_TIME</code> variable.</p>"},{"location":"archive/troubleshooting-old/#114-troubleshooting-runtime-problems-from-cime-docs","title":"11.4. Troubleshooting runtime problems (FROM CIME DOCS)","text":"<p>To see if a run completed successfully, check the last several lines of the cpl.log file for a string like SUCCESSFUL TERMINATION. A successful job also usually copies the log files to the $CASEROOT/logs directory.</p> <p>Check these things first when a job fails:</p> <p>Did the model time out?</p> <p>Was a disk quota limit hit?</p> <p>Did a machine go down?</p> <p>Did a file system become full?</p> <p>If any of those things happened, take appropriate corrective action (see suggestions below) and resubmit the job.</p> <p>If it is not clear that any of the above caused a case to fail, there are several places to look for error messages.</p> <p>Check component log files in your run directory ($RUNDIR). This directory is set in the env_run.xml file. Each component writes its own log file, and there should be log files for every component in this format: cpl.log.yymmdd-hhmmss. Check each log file for an error message, especially at or near the end.</p> <p>Check for a standard out and/or standard error file in $CASEROOT. The standard out/err file often captures a significant amount of extra model output and also often contains significant system output when a job terminates. Useful error messages sometimes are found well above the bottom of a large standard out/err file. Backtrack from the bottom in search of an error message.</p> <p>Check for core files in your run directory and review them using an appropriate tool.</p> <p>Check any automated email from the job about why a job failed. Some sites\u2019 batch schedulers send these.</p> <p>Check the archive directory: $DOUT_S_ROOT/$CASE. If a case failed, the log files or data may still have been archived.</p> <p>Common errors</p> <p>One common error is for a job to time out, which often produces minimal error messages. Review the daily model date stamps in the cpl.log file and the timestamps of files in your run directory to deduce the start and stop time of a run. If the model was running fine, but the wallclock limit was reached, either reduce the run length or increase the wallclock setting.</p> <p>If the model hangs and then times out, that usually indicates an MPI or file system problem or possibly a model problem. If you suspect an intermittent system problem, try resubmitting the job. Also send a help request to local site consultants to provide them with feedback about system problems and to get help.</p> <p>Another error that can cause a timeout is a slow or intermittently slow node. The cpl.log file normally outputs the time used for every model simulation day. To review that data, grep the cpl.log file for the string tStamp as shown here:</p> <p>grep tStamp cpl.log.* | more The output looks like this:</p> <p>tStamp_write: model date = 10120 0 wall clock = 2009-09-28 09:10:46 avg dt = 58.58 dt = 58.18 tStamp_write: model date = 10121 0 wall clock = 2009-09-28 09:12:32 avg dt = 60.10 dt = 105.90 Review the run times at the end of each line for each model day. The \u201cavg dt =\u201d is the average time to simulate a model day and \u201cdt = \u201c is the time needed to simulate the latest model day.</p> <p>The model date is printed in YYYYMMDD format and the wallclock is the local date and time. In the example, 10120 is Jan 20, 0001, and the model took 58 seconds to run that day. The next day, Jan 21, took 105.90 seconds.</p> <p>A wide variation in the simulation time for typical mid-month model days suggests a system problem. However, there are variations in the cost of the model over time. For instance, on the last day of every simulated month, the model typically writes netcdf files, which can be a significant intermittent cost. Also, some model configurations read data mid-month or run physics intermittently at a timestep longer than one day. In those cases, some variability is expected. The time variation typically is quite erratic and unpredictable if the problem is system performance variability.</p> <p>Sometimes when a job times out or overflows disk space, the restart files will get mangled. With the exception of the CAM and CLM history files, all the restart files have consistent sizes.</p> <p>Compare the restart files against the sizes of a previous restart. If they don\u2019t match, remove them and move the previous restart into place before resubmitting the job. See Restarting a run.</p> <p>It is not uncommon for nodes to fail on HPC systems or for access to large file systems to hang. Before you file a bug report, make sure a case fails consistently in the same place.</p> <p>Rerunning with additional debugging information</p> <p>There are a few changes you can make to your case to get additional information that aids in debugging:</p> <p>Increase the value of the run-time xml variable INFO_DBUG: ./xmlchange INFO_DBUG=2. This adds more information to the cpl.log file that can be useful if you can\u2019t tell what component is aborting the run, or where bad coupling fields are originating. (This does NOT require rebuilding.)</p> <p>Try rebuilding and rerunning with the build-time xml variable DEBUG set to TRUE: ./xmlchange DEBUG=TRUE.</p> <p>This adds various runtime checks that trap conditions such as out-of-bounds array indexing, divide by 0, and other floating point exceptions (the exact conditions checked depend on flags set in macros defined in the cmake_macros subdirectory of the caseroot).</p> <p>The best way to do this is often to create a new case and run ./xmlchange DEBUG=TRUE before running ./case.build. However, if it is hard for you to recreate your case, then you can run that xmlchange command from your existing case; then you must run ./case.build --clean-all before rerunning ./case.build.</p> <p>Note that the model will run significantly slower in this mode, so this may not be feasible if the model has to run a long time before producing the error. (Sometimes it works well to run the model until shortly before the error in non-debug mode, have it write restart files, then restart after rebuilding in debug mode.) Also note that answers will change slightly, so if the error arises from a rare condition, then it may not show up in this mode.</p>"},{"location":"installation/cime-config/","title":"CIME Configuration","text":"<p>CTSM uses CIME (Common Infrastructure for Modeling the Earth) for building and running cases. This page explains the HiPerGator-specific configuration and what you may need to customize.</p>"},{"location":"installation/cime-config/#how-cime-finds-machine-config","title":"How CIME Finds Machine Config","text":"<p>CIME looks for machine configurations in this order:</p> <ol> <li><code>~/.cime/</code> (user overrides)</li> <li><code>$CTSMROOT/ccs_config/machines/&lt;machine&gt;/</code> (machine-specific)</li> <li><code>$CTSMROOT/ccs_config/machines/</code> (defaults)</li> </ol> <p>Our fork includes HiPerGator configs at: <pre><code>ccs_config/machines/hipergator/\n\u251c\u2500\u2500 config_machines.xml    # Machine definition\n\u251c\u2500\u2500 config_batch.xml       # SLURM settings\n\u2514\u2500\u2500 gnu_hipergator.cmake   # Compiler flags\n</code></pre></p>"},{"location":"installation/cime-config/#configuration-files","title":"Configuration Files","text":""},{"location":"installation/cime-config/#config_machinesxml","title":"config_machines.xml","text":"<p>The main machine definition. Key sections:</p>"},{"location":"installation/cime-config/#basic-system-info","title":"Basic System Info","text":"<pre><code>&lt;DESC&gt;UF HiPerGator | hpg-default node | AMD ROME 128 cores per node | slurm&lt;/DESC&gt;\n&lt;OS&gt;LINUX&lt;/OS&gt;\n&lt;COMPILERS&gt;gnu&lt;/COMPILERS&gt;\n&lt;MPILIBS&gt;openmpi&lt;/MPILIBS&gt;\n</code></pre>"},{"location":"installation/cime-config/#file-paths","title":"File Paths","text":"<p>These define where CTSM reads/writes data:</p> <pre><code>&lt;CIME_OUTPUT_ROOT&gt;/blue/gerber/$ENV{USER}/earth_model_output/cime_output_root&lt;/CIME_OUTPUT_ROOT&gt;\n&lt;DIN_LOC_ROOT&gt;/blue/gerber/earth_models/inputdata&lt;/DIN_LOC_ROOT&gt;\n&lt;DOUT_S_ROOT&gt;$CIME_OUTPUT_ROOT/archive/$CASE&lt;/DOUT_S_ROOT&gt;\n</code></pre> Variable Purpose <code>CIME_OUTPUT_ROOT</code> Where case build/run directories are created <code>DIN_LOC_ROOT</code> Where CTSM looks for (and downloads) input data <code>DOUT_S_ROOT</code> Where output is archived after runs complete <p>You Must Customize These</p> <p>Change <code>/blue/gerber/</code> to your group's allocation path.</p>"},{"location":"installation/cime-config/#node-configuration","title":"Node Configuration","text":"<pre><code>&lt;MAX_TASKS_PER_NODE&gt;128&lt;/MAX_TASKS_PER_NODE&gt;\n&lt;MAX_MPITASKS_PER_NODE&gt;128&lt;/MAX_MPITASKS_PER_NODE&gt;\n</code></pre> <p>HiPerGator's default nodes have 128 cores. This tells CIME how to distribute MPI tasks.</p>"},{"location":"installation/cime-config/#module-system","title":"Module System","text":"<p>The config tells CIME which modules to load:</p> <pre><code>&lt;modules&gt;\n  &lt;command name=\"load\"&gt;cmake/3.26.4&lt;/command&gt;\n  &lt;command name=\"load\"&gt;gcc/14.2.0&lt;/command&gt;\n  &lt;command name=\"load\"&gt;openmpi/5.0.7&lt;/command&gt;\n  &lt;!-- ... more modules ... --&gt;\n&lt;/modules&gt;\n</code></pre> <p>These must match your <code>ctsm-modules</code> collection.</p>"},{"location":"installation/cime-config/#environment-variables","title":"Environment Variables","text":"<pre><code>&lt;environment_variables&gt;\n  &lt;env name=\"NETCDF_PATH\"&gt;$ENV{HPC_NETCDF_C_DIR}&lt;/env&gt;\n  &lt;env name=\"PIO\"&gt;/blue/gerber/earth_models/shared/parallelio/bld&lt;/env&gt;\n  &lt;!-- ... more variables ... --&gt;\n&lt;/environment_variables&gt;\n</code></pre> <p>PIO Path</p> <p>The PIO paths point to the shared ParallelIO build. Update these to your group's build location.</p>"},{"location":"installation/cime-config/#config_batchxml","title":"config_batch.xml","text":"<p>SLURM job submission settings:</p> <pre><code>&lt;batch_system MACH=\"hipergator\" type=\"slurm\"&gt;\n  &lt;batch_submit&gt;sbatch&lt;/batch_submit&gt;\n  &lt;submit_args&gt;\n    &lt;arg flag=\"--time\" name=\"$JOB_WALLCLOCK_TIME\"/&gt;\n    &lt;arg flag=\"-q\" name=\"$JOB_QUEUE\"/&gt;\n    &lt;arg flag=\"--mem\" name=\"16GB\"/&gt;\n  &lt;/submit_args&gt;\n  &lt;directives&gt;\n    &lt;directive&gt;--partition=hpg-default&lt;/directive&gt;\n    &lt;directive&gt;--mail-type=NONE&lt;/directive&gt;\n  &lt;/directives&gt;\n  &lt;queues&gt;\n    &lt;queue jobmin=\"1\" jobmax=\"20\" default=\"true\"&gt;gerber&lt;/queue&gt;\n    &lt;queue jobmin=\"1\" jobmax=\"128\"&gt;gerber-b&lt;/queue&gt;\n  &lt;/queues&gt;\n&lt;/batch_system&gt;\n</code></pre>"},{"location":"installation/cime-config/#key-settings","title":"Key Settings","text":"<p>No <code>--exclusive</code> flag: Unlike many HPC centers, HiPerGator shares nodes by default. We don't include <code>--exclusive</code> because: - Most CTSM runs use few cores - Exclusive access wastes resources - QoS limits often don't allow full-node jobs anyway</p> <p>Memory request: Fixed at 16GB. Adjust if your runs need more.</p> <p>Queues section:</p> <p>You Must Change This</p> <p>The queue names (<code>gerber</code>, <code>gerber-b</code>) are QoS names specific to the Gerber group. Replace these with your group's QoS names.</p> <p>Check your group's QoS: <pre><code>showQos -A &lt;your_group&gt;\n</code></pre></p> <p>The <code>jobmin</code> and <code>jobmax</code> values limit task counts for each queue.</p>"},{"location":"installation/cime-config/#gnu_hipergatorcmake","title":"gnu_hipergator.cmake","text":"<p>Compiler flags and library linking:</p> <pre><code># NetCDF legacy macro support\nstring(APPEND CPPDEFS \" -DNETCDF_ENABLE_LEGACY_MACROS\")\n\n# Link netcdf and lapack\nstring(APPEND SLIBS \" ${SHELL_CMD_OUTPUT_BUILD_INTERNAL_IGNORE0}\")\nstring(APPEND SLIBS \" -L$(LAPACK_LIBDIR) -llapack -lblas\")\n\n# Lustre filesystem optimization\nset(PIO_FILESYSTEM_HINTS \"lustre\")\n</code></pre> <p>You probably won't need to modify this file.</p>"},{"location":"installation/cime-config/#customizing-for-your-group","title":"Customizing for Your Group","text":""},{"location":"installation/cime-config/#required-changes","title":"Required Changes","text":"<ol> <li>File paths in <code>config_machines.xml</code>:</li> <li><code>CIME_OUTPUT_ROOT</code>: Change <code>/blue/gerber/</code> to <code>/blue/&lt;your_group&gt;/</code></li> <li><code>DIN_LOC_ROOT</code>: Point to your group's inputdata location</li> <li> <p><code>PIO</code> paths: Point to your group's ParallelIO build</p> </li> <li> <p>Queue names in <code>config_batch.xml</code>:</p> </li> <li>Replace <code>gerber</code> and <code>gerber-b</code> with your group's QoS names</li> </ol>"},{"location":"installation/cime-config/#optional-changes","title":"Optional Changes","text":"<ul> <li>Memory: Increase <code>--mem</code> in <code>config_batch.xml</code> if needed</li> <li>Email: Change mail settings in <code>config_batch.xml</code></li> <li>Partition: Change from <code>hpg-default</code> if using GPU nodes</li> </ul>"},{"location":"installation/cime-config/#making-changes","title":"Making Changes","text":"<p>After modifying config files:</p> <pre><code># If you already have cases built, they may need rebuilding\ncd /path/to/your/case\n./case.build --clean-all\n./case.build\n</code></pre> <p>New cases will automatically use the updated configs.</p>"},{"location":"installation/cime-config/#the-cime-alternative","title":"The ~/.cime Alternative","text":"<p>CIME supports user-level config overrides in <code>~/.cime/</code>. You could theoretically:</p> <ol> <li>Clone upstream CTSM (no fork needed)</li> <li>Put HiPerGator configs in <code>~/.cime/</code></li> </ol> <p>Why we don't recommend this:</p> <ul> <li>Batch config loading has bugs in CIME v3</li> <li><code>NODENAME_REGEX</code> machine detection is unreliable</li> <li>Each user must maintain their own configs</li> <li>Harder to share and troubleshoot</li> </ul> <p>The fork approach gives everyone consistent, working configs.</p>"},{"location":"installation/cime-config/#troubleshooting","title":"Troubleshooting","text":"<p>\"Machine not found\" : Check that <code>ccs_config/machines/hipergator/</code> exists and submodules are initialized</p> <p>Job submission fails : Verify queue names match your group's QoS</p> <p>Build can't find libraries : Check that module versions in config match your <code>ctsm-modules</code> collection</p> <p>Wrong output location : Check <code>CIME_OUTPUT_ROOT</code> path in <code>config_machines.xml</code></p>"},{"location":"installation/fork-setup/","title":"Fork Reference","text":"<p>This page explains why and how we forked CTSM for HiPerGator. Use this as a reference for understanding our modifications or for creating your own fork.</p>"},{"location":"installation/fork-setup/#why-we-fork","title":"Why We Fork","text":"<p>CTSM isn't plug-and-play on HiPerGator. Several issues require source-level modifications.</p>"},{"location":"installation/fork-setup/#1-machine-configuration-ccs_config","title":"1. Machine Configuration (ccs_config)","text":"<p>CTSM uses CIME (Common Infrastructure for Modeling the Earth) for builds and job submission. CIME needs machine-specific configuration files that define:</p> <ul> <li>Compiler paths and flags</li> <li>MPI library settings</li> <li>SLURM batch directives</li> <li>Input data locations</li> </ul> <p>HiPerGator isn't in the upstream machine list, so we need custom configs.</p> <p>The complication: Machine configs live in a separate repository (<code>ccs_config_cesm</code>) that CTSM pulls in as a git submodule. To add HiPerGator support, we had to:</p> <ol> <li>Fork <code>ccs_config_cesm</code></li> <li>Add HiPerGator configs to our fork</li> <li>Fork CTSM to point its <code>.gitmodules</code> at our ccs_config fork</li> </ol> <p>This submodule-of-a-submodule situation makes the fork necessary.</p>"},{"location":"installation/fork-setup/#2-user-config-cime-doesnt-work-reliably","title":"2. User Config (~/.cime) Doesn't Work Reliably","text":"<p>CIME v3 theoretically supports user config overrides in <code>~/.cime/</code>. We tried this approach and encountered bugs:</p> <ul> <li>Batch configuration loading was inconsistent</li> <li><code>NODENAME_REGEX</code> machine detection failed intermittently</li> <li>Each user had to maintain their own config files</li> </ul> <p>Forking ccs_config gives us a single source of truth that works reliably for everyone.</p>"},{"location":"installation/fork-setup/#3-build-tool-bugs","title":"3. Build Tool Bugs","text":"<p>Several CTSM build tools have issues with newer GCC versions:</p> Issue Symptom Fix PIO linking mksurfdata fails to link Change <code>STATIC</code> to <code>SHARED</code> in CMakeLists.txt Format specifiers GCC 10+ compilation errors Add explicit width to Fortran format specs (<code>I</code> \u2192 <code>I12</code>) GCC 14 flags Compilation warnings as errors Add <code>-fallow-argument-mismatch</code> flag <p>These are bugs that should be fixed upstream, but until they are, our fork includes the fixes.</p>"},{"location":"installation/fork-setup/#4-hipergator-specific-paths","title":"4. HiPerGator-Specific Paths","text":"<p>Default paths in some tools point to NCAR systems. Our fork updates paths to work with HiPerGator's storage layout.</p>"},{"location":"installation/fork-setup/#repository-structure","title":"Repository Structure","text":"Repository Branch Purpose cdevaneprugh/CTSM <code>uf-ctsm5.3.085</code> CTSM with tool fixes cdevaneprugh/ccs_config_cesm <code>uf-hipergator</code> Machine configuration <p>The ccs_config fork is included as a submodule of the CTSM fork.</p> <p>Version Notice</p> <p>These forks are based on CTSM 5.3.085. Other versions may have different issues or may have fixed some of these bugs upstream.</p>"},{"location":"installation/fork-setup/#modifications-detail","title":"Modifications Detail","text":""},{"location":"installation/fork-setup/#ctsm-modifications","title":"CTSM Modifications","text":"File Change Reason <code>tools/mksurfdata_esmf/src/CMakeLists.txt</code> <code>STATIC</code> \u2192 <code>SHARED</code> Fix for shared PIO libraries <code>tools/mksurfdata_esmf/src/mksurfdata.F90</code> Format specifiers <code>I</code> \u2192 <code>I12</code> GCC 10+ requires explicit widths <code>tools/mksurfdata_esmf/gen_mksurfdata_build</code> Added GCC 14 flags Compatibility workarounds <code>python/ctsm/site_and_regional/single_point_case.py</code> <code>MPILIB=openmpi</code> HiPerGator uses OpenMPI <code>tools/site_and_regional/default_data_*.cfg</code> Input paths HiPerGator data locations"},{"location":"installation/fork-setup/#ccs_config-modifications","title":"ccs_config Modifications","text":"<p>The <code>machines/hipergator/</code> directory contains:</p> <ul> <li>config_machines.xml - Full machine definition (compilers, paths, MPI settings)</li> <li>config_batch.xml - SLURM batch settings (no <code>--exclusive</code> flag for shared nodes)</li> <li>gnu_hipergator.cmake - Compiler flags and library paths</li> </ul> <p>See CIME Configuration for detailed explanation of these files.</p>"},{"location":"installation/fork-setup/#using-this-fork","title":"Using This Fork","text":""},{"location":"installation/fork-setup/#option-1-clone-directly","title":"Option 1: Clone Directly","text":"<p>The simplest approach - clone our fork and use it as-is. See Quick Start for installation steps.</p> <p>You'll need to customize the QoS settings in the config files for your group (see CIME Configuration).</p>"},{"location":"installation/fork-setup/#option-2-fork-from-us","title":"Option 2: Fork from Us","text":"<p>If you need additional modifications:</p> <ol> <li>Fork <code>cdevaneprugh/CTSM</code> on GitHub</li> <li>Clone your fork</li> <li>Make your modifications</li> <li>Update the ccs_config submodule if needed</li> </ol> <p>This gives you a customizable base with our fixes already applied.</p>"},{"location":"installation/fork-setup/#option-3-start-from-upstream","title":"Option 3: Start from Upstream","text":"<p>If you need a different CTSM version or want full control:</p> <ol> <li>Fork <code>ESCOMP/CTSM</code> on GitHub</li> <li>Fork <code>ESMCI/ccs_config_cesm</code></li> <li>Apply the modifications listed above</li> <li>Update <code>.gitmodules</code> to point to your ccs_config fork</li> </ol> <p>This is more work but gives you a clean slate.</p>"},{"location":"installation/fork-setup/#updating-the-fork","title":"Updating the Fork","text":"<p>When upstream releases a new CTSM version:</p> <pre><code># Add upstream remote (one time)\ngit remote add upstream https://github.com/ESCOMP/CTSM.git\n\n# Fetch new tags\ngit fetch upstream --tags\n\n# Create new branch from new tag\ngit checkout -b uf-ctsm5.3.090 ctsm5.3.090\n\n# Cherry-pick modifications from old branch\ngit cherry-pick &lt;commit-hash&gt;...\n\n# Update submodules\n./bin/git-fleximod update\n\n# Test and push\ngit push -u origin uf-ctsm5.3.090\n</code></pre> <p>Check if fixes are still needed</p> <p>Before cherry-picking, check if upstream has fixed the issues. You may not need all the modifications for newer versions.</p>"},{"location":"installation/mksurfdata/","title":"Building mksurfdata","text":"<p>The <code>mksurfdata</code> tool generates surface datasets (fsurdat files) for CTSM. This page covers building the tool on HiPerGator.</p>"},{"location":"installation/mksurfdata/#prerequisites","title":"Prerequisites","text":"<p>Before building mksurfdata, ensure you have:</p> <ol> <li>Module collection loaded: <code>module restore ctsm-modules</code></li> <li>PIO library built and path set (see Prerequisites)</li> <li>CTSM fork cloned (fixes are already applied)</li> </ol>"},{"location":"installation/mksurfdata/#building-the-executable","title":"Building the Executable","text":"<pre><code># Load modules\nmodule restore ctsm-modules\n\n# Set PIO path (customize for your group)\nexport PIO=\"/blue/&lt;group&gt;/earth_models/shared/parallelio/bld\"\n\n# Navigate to mksurfdata directory\ncd $CTSMROOT/tools/mksurfdata_esmf\n\n# Build for HiPerGator\n./gen_mksurfdata_build --machine hipergator\n</code></pre> <p>Expected output: <pre><code>Successfully created mksurfdata_esmf executable for: hipergator_gnu for openmpi library\n</code></pre></p>"},{"location":"installation/mksurfdata/#verify-the-build","title":"Verify the Build","text":"<p>Check that PIO libraries are properly linked:</p> <pre><code>cd tool_bld\nldd mksurfdata | grep libpio\n</code></pre> <p>Expected output (paths will match your PIO location): <pre><code>libpiof.so =&gt; /blue/&lt;group&gt;/earth_models/shared/parallelio/bld/lib/libpiof.so\nlibpioc.so =&gt; /blue/&lt;group&gt;/earth_models/shared/parallelio/bld/lib/libpioc.so\n</code></pre></p> <p>If you see \"not found\", the libraries weren't linked correctly.</p>"},{"location":"installation/mksurfdata/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/mksurfdata/#build-errors","title":"Build Errors","text":"<p>If the build fails:</p> <ol> <li>Delete the build directory: <code>rm -rf tool_bld</code></li> <li>Verify modules are loaded: <code>module list</code></li> <li>Verify PIO path: <code>ls $PIO/lib/libpiof.so</code></li> <li>Retry the build</li> </ol>"},{"location":"installation/mksurfdata/#common-issues","title":"Common Issues","text":"Error Cause Solution \"libpio not found\" PIO not in path Set <code>export PIO=...</code> before building Format string errors Using unfixed upstream Use our fork with fixes applied CMake errors Wrong module versions Restore <code>ctsm-modules</code> collection"},{"location":"installation/mksurfdata/#fixes-applied-in-our-fork","title":"Fixes Applied in Our Fork","text":"<p>The upstream mksurfdata has bugs that prevent building on HiPerGator. Our fork includes these fixes:</p>"},{"location":"installation/mksurfdata/#1-cmakeliststxt-static-vs-shared","title":"1. CMakeLists.txt - STATIC vs SHARED","text":"<p>Problem: Declares libraries as STATIC but points to shared <code>.so</code> files.</p> <p>Fix: Change <code>STATIC IMPORTED</code> to <code>SHARED IMPORTED</code>:</p> <pre><code># Before (buggy)\nadd_library(pioc STATIC IMPORTED)\nadd_library(piof STATIC IMPORTED)\n\n# After (fixed)\nadd_library(pioc SHARED IMPORTED)\nadd_library(piof SHARED IMPORTED)\n</code></pre> <p>File: <code>tools/mksurfdata_esmf/src/CMakeLists.txt</code> lines 51-52</p>"},{"location":"installation/mksurfdata/#2-mksurfdataf90-format-specifiers","title":"2. mksurfdata.F90 - Format Specifiers","text":"<p>Problem: GCC 10+ requires explicit width in format specifiers. The <code>I</code> format without width is a legacy extension.</p> <p>Fix: Add explicit widths:</p> <pre><code>! Before (buggy)\nwrite(ndiag,'(2(a,I))') ' npes = ', npes, ' grid size = ', grid_size\n\n! After (fixed)\nwrite(ndiag,'(2(a,I12))') ' npes = ', npes, ' grid size = ', grid_size\n</code></pre> <p>File: <code>tools/mksurfdata_esmf/src/mksurfdata.F90</code> lines 271, 295, 328</p>"},{"location":"installation/mksurfdata/#3-gen_mksurfdata_build-gcc-14-flags","title":"3. gen_mksurfdata_build - GCC 14 Flags","text":"<p>Problem: GCC 14 is stricter about legacy Fortran code.</p> <p>Fix: Add compatibility flags:</p> <pre><code>-DCMAKE_Fortran_FLAGS=\" -fallow-argument-mismatch -fallow-invalid-boz -ffree-line-length-none\"\n</code></pre> <p>File: <code>tools/mksurfdata_esmf/gen_mksurfdata_build</code> line 170</p>"},{"location":"installation/mksurfdata/#python-environment","title":"Python Environment","text":"<p>Some mksurfdata workflows require the CTSM Python environment:</p> <pre><code>module load conda\ncd $CTSMROOT\n./py_env_create\nconda activate ctsm_pylib\n</code></pre> <p>This creates a conda environment with CTSM's Python dependencies.</p>"},{"location":"installation/mksurfdata/#next-steps","title":"Next Steps","text":"<p>After building mksurfdata:</p> <ul> <li>See Single-Point Runs for using subset data</li> <li>See the CTSM documentation for creating custom surface datasets</li> </ul>"},{"location":"installation/prerequisites/","title":"Prerequisites","text":"<p>Required modules, libraries, and paths for running CTSM on HiPerGator.</p>"},{"location":"installation/prerequisites/#directory-structure","title":"Directory Structure","text":"<p>Each research group should set up a directory structure for CTSM resources. Here's a recommended layout:</p> <pre><code>/blue/&lt;group&gt;/earth_models/\n\u251c\u2500\u2500 ctsm5.3/              # CTSM installation (your fork or clone)\n\u251c\u2500\u2500 inputdata/            # Global input data (downloaded from NCAR)\n\u251c\u2500\u2500 shared/\n\u2502   \u251c\u2500\u2500 cprnc/            # NetCDF comparison tool\n\u2502   \u2514\u2500\u2500 parallelio/       # PIO library for mksurfdata\n\u2514\u2500\u2500 shared.subset.data/   # Subset data for single-point runs\n</code></pre> <p>Input Data Storage</p> <p>CTSM downloads input data on-demand from NCAR servers. This can grow to multiple terabytes over time. Plan your storage allocation accordingly, and coordinate with your group to avoid duplicate downloads.</p>"},{"location":"installation/prerequisites/#module-environment","title":"Module Environment","text":"<p>CTSM requires specific compiler and library versions. We maintain a module collection:</p>"},{"location":"installation/prerequisites/#setting-up-the-module-collection","title":"Setting Up the Module Collection","text":"<pre><code>module purge\n\n# Load required modules\nmodule load subversion/1.9.7\nmodule load perl/5.24.1\nmodule load cmake/3.26.4\nmodule load python/3.12\nmodule load gcc/14.2.0\nmodule load lapack/3.11.0\nmodule load openmpi/5.0.7\nmodule load netcdf-c/4.9.3 netcdf-f/4.6.2\nmodule load hdf5\nmodule load esmf/8.8.1\n\n# Save the collection\nmodule save ctsm-modules\n</code></pre>"},{"location":"installation/prerequisites/#using-the-collection","title":"Using the Collection","text":"<pre><code># Restore modules (do this at the start of each session)\nmodule restore ctsm-modules\n\n# Verify\nmodule list\n</code></pre>"},{"location":"installation/prerequisites/#shared-libraries","title":"Shared Libraries","text":"<p>These libraries need to be built once per group and can be shared among group members.</p>"},{"location":"installation/prerequisites/#cprnc-netcdf-comparison-tool","title":"cprnc (NetCDF Comparison Tool)","text":"<p>Used to compare NetCDF output files. Useful for validating model output.</p>"},{"location":"installation/prerequisites/#parallelio-pio","title":"ParallelIO (PIO)","text":"<p>Required for building the mksurfdata tool. Set the <code>PIO</code> environment variable to point to your build:</p> <pre><code>export PIO=\"/blue/&lt;group&gt;/earth_models/shared/parallelio/bld\"\n</code></pre>"},{"location":"installation/prerequisites/#building-shared-libraries","title":"Building Shared Libraries","text":"<p>Build these once for your group. Each group member can then use the same installation.</p>"},{"location":"installation/prerequisites/#building-cprnc","title":"Building cprnc","text":"<pre><code>module restore ctsm-modules\n\ngit clone https://github.com/ESMCI/cprnc.git\ncd cprnc\nmkdir bld &amp;&amp; cd bld\ncmake ../\nmake\n</code></pre>"},{"location":"installation/prerequisites/#building-pio","title":"Building PIO","text":"<pre><code>module restore ctsm-modules\n\ngit clone https://github.com/NCAR/ParallelIO.git parallelio\ncd parallelio\nmkdir bld\n\ncmake \\\n  -DNetCDF_C_PATH=/apps/gcc/14.2.0/openmpi/5.0.7/netcdf-c/4.9.3 \\\n  -DNetCDF_Fortran_PATH=/apps/gcc/14.2.0/openmpi/5.0.7/netcdf-f/4.6.2 \\\n  -DWITH_PNETCDF=OFF \\\n  -DBUILD_SHARED_LIBS=ON \\\n  -DCMAKE_INSTALL_PREFIX=`pwd`/bld \\\n  -DCMAKE_INSTALL_RPATH=`pwd`/src/gptl \\\n  -DCMAKE_INSTALL_RPATH_USE_LINK_PATH=TRUE \\\n  ../\n\nmake\nmake install\n</code></pre>"},{"location":"installation/prerequisites/#environment-variables","title":"Environment Variables","text":"<p>See Onboarding for recommended environment variables. Key variables for prerequisites:</p> <pre><code># Shared libraries (customize path)\nexport PIO=\"/blue/&lt;group&gt;/earth_models/shared/parallelio/bld\"\n</code></pre>"},{"location":"installation/prerequisites/#verification","title":"Verification","text":"<p>After building the shared libraries, verify your setup:</p> <pre><code># Check modules\nmodule restore ctsm-modules\nmodule list\n\n# Check PIO was built correctly\nls $PIO/lib/libpiof.so\n</code></pre>"},{"location":"installation/quickstart/","title":"Quick Start","text":"<p>Get CTSM running on HiPerGator. This guide covers the minimum steps to clone, build, and run your first case.</p>"},{"location":"installation/quickstart/#before-you-begin","title":"Before You Begin","text":"<p>You'll need:</p> <ul> <li>HiPerGator account with group allocation on <code>/blue/</code></li> <li>Basic familiarity with the Linux command line</li> <li>~50GB free space for CTSM installation (input data is separate)</li> </ul> <p>New to HiPerGator? See Onboarding first.</p>"},{"location":"installation/quickstart/#step-1-set-up-modules","title":"Step 1: Set Up Modules","text":"<p>CTSM requires specific compiler and library versions. Create a module collection:</p> <pre><code>module purge\n\n# Load required modules\nmodule load subversion/1.9.7\nmodule load perl/5.24.1\nmodule load cmake/3.26.4\nmodule load python/3.12\nmodule load gcc/14.2.0\nmodule load lapack/3.11.0\nmodule load openmpi/5.0.7\nmodule load netcdf-c/4.9.3 netcdf-f/4.6.2\nmodule load hdf5\nmodule load esmf/8.8.1\n\n# Save the collection for future sessions\nmodule save ctsm-modules\n</code></pre> <p>From now on, just run <code>module restore ctsm-modules</code> to load all modules.</p>"},{"location":"installation/quickstart/#step-2-clone-ctsm","title":"Step 2: Clone CTSM","text":"<p>We maintain a fork with HiPerGator-specific fixes. Clone it:</p> <pre><code>cd /blue/&lt;group&gt;/$USER\n\n# Clone from our fork\ngit clone https://github.com/cdevaneprugh/CTSM.git ctsm5.3\ncd ctsm5.3\n\n# Checkout the HiPerGator branch\ngit checkout uf-ctsm5.3.085\n</code></pre> <p>Replace <code>&lt;group&gt;</code> with your research group's allocation name.</p>"},{"location":"installation/quickstart/#step-3-initialize-submodules","title":"Step 3: Initialize Submodules","text":"<p>CTSM uses <code>git-fleximod</code> to manage submodules (including the machine configuration):</p> <pre><code>./bin/git-fleximod update\n</code></pre> <p>This downloads all required components. It may take a few minutes.</p>"},{"location":"installation/quickstart/#step-4-verify-installation","title":"Step 4: Verify Installation","text":"<p>Check that everything is in place:</p> <pre><code># Check submodule status\n./bin/git-fleximod status\n\n# Verify HiPerGator config exists\nls ccs_config/machines/hipergator/\n</code></pre> <p>You should see: <code>config_machines.xml</code>, <code>config_batch.xml</code>, <code>gnu_hipergator.cmake</code></p>"},{"location":"installation/quickstart/#step-5-customize-for-your-group","title":"Step 5: Customize for Your Group","text":"<p>The HiPerGator config includes QoS limits that need to match your group's allocation. Check your group's limits:</p> <pre><code>showQos -A &lt;group&gt;\n</code></pre> <p>Then edit the config file to match (see CIME Configuration for details):</p> <pre><code>vim ccs_config/machines/hipergator/config_machines.xml\n</code></pre> <p>Look for the <code>MAX_TASKS_PER_NODE</code> and <code>MAX_MPITASKS_PER_NODE</code> settings.</p>"},{"location":"installation/quickstart/#step-6-create-a-test-case","title":"Step 6: Create a Test Case","text":"<p>Test your installation with a simple case:</p> <pre><code>cd cime/scripts\n\n./create_newcase \\\n    --case /blue/&lt;group&gt;/$USER/cases/test_case \\\n    --compset I2000Clm60Sp \\\n    --res f45_f45_mg37 \\\n    --machine hipergator \\\n    --run-unsupported\n</code></pre>"},{"location":"installation/quickstart/#step-7-build-and-run","title":"Step 7: Build and Run","text":"<pre><code>cd /blue/&lt;group&gt;/$USER/cases/test_case\n\n# Set up the case\n./case.setup\n\n# Build (this takes ~20 minutes the first time)\n./case.build\n\n# Submit to the queue\n./case.submit\n</code></pre>"},{"location":"installation/quickstart/#step-8-check-results","title":"Step 8: Check Results","text":"<p>Monitor your job:</p> <pre><code>squeue -u $USER\n</code></pre> <p>Check for output:</p> <pre><code># Find your run directory\n./xmlquery RUNDIR\n\n# List output files\nls $(./xmlquery --value RUNDIR)/*.nc\n</code></pre>"},{"location":"installation/quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>Case Workflow - Detailed case configuration</li> <li>Single-Point Runs - Site-specific simulations</li> <li>Prerequisites - Build additional tools (mksurfdata, cprnc)</li> <li>CIME Configuration - Understand and customize machine config</li> <li>Fork Reference - Why we forked and what's modified</li> </ul>"},{"location":"installation/quickstart/#troubleshooting","title":"Troubleshooting","text":"<p>Build fails immediately : Check that modules are loaded: <code>module list</code></p> <p>Job stays pending : Check QoS limits match your config: <code>showQos -A &lt;group&gt;</code></p> <p>Missing machine config : Verify submodules initialized: <code>./bin/git-fleximod status</code></p> <p>See Troubleshooting for more help.</p>"},{"location":"reference/modifications/","title":"Fork Modifications","text":"<p>This page documents modifications made in our CTSM fork and their rationale.</p>"},{"location":"reference/modifications/#overview","title":"Overview","text":"<p>Our forks include HiPerGator-specific changes and bug fixes for building tools:</p> Repository Branch Purpose cdevaneprugh/CTSM <code>uf-ctsm5.3.085</code> Tool fixes, paths cdevaneprugh/ccs_config_cesm <code>uf-hipergator</code> Machine config"},{"location":"reference/modifications/#ctsm-modifications","title":"CTSM Modifications","text":""},{"location":"reference/modifications/#1-cmakeliststxt-static-vs-shared","title":"1. CMakeLists.txt - STATIC vs SHARED","text":"<p>File: <code>tools/mksurfdata_esmf/src/CMakeLists.txt</code></p> <p>Problem: Upstream declares libraries as <code>STATIC IMPORTED</code> but points to shared <code>.so</code> files:</p> <pre><code># Buggy upstream code\nadd_library(pioc STATIC IMPORTED)\nset_property(TARGET pioc PROPERTY IMPORTED_LOCATION $ENV{PIO}/lib/libpioc.so)\n</code></pre> <p>This is an internal inconsistency - you cannot declare <code>STATIC</code> and point to a shared library.</p> <p>Fix: Change to <code>SHARED IMPORTED</code>:</p> <pre><code>add_library(pioc SHARED IMPORTED)\nadd_library(piof SHARED IMPORTED)\n</code></pre> <p>Why it works at NCAR: Their systems likely have PIO as static libraries, or use different CMake handling.</p> <p>Contribution candidate: Yes - genuine bug affecting anyone using shared PIO.</p>"},{"location":"reference/modifications/#2-mksurfdataf90-format-specifiers","title":"2. mksurfdata.F90 - Format Specifiers","text":"<p>File: <code>tools/mksurfdata_esmf/src/mksurfdata.F90</code></p> <p>Problem: GCC 10+ requires explicit width in Fortran format specifiers. The <code>I</code> format without width is a legacy extension:</p> <pre><code>! Buggy - no width specified\nwrite(ndiag,'(2(a,I))') ' npes = ', npes, ' grid size = ', grid_size\n</code></pre> <p>Fix: Add explicit widths:</p> <pre><code>write(ndiag,'(2(a,I12))') ' npes = ', npes, ' grid size = ', grid_size\n</code></pre> <p>Lines modified: 271, 295, 328</p> <p>Why it works at NCAR: They likely use Intel compilers or older GCC versions.</p> <p>Contribution candidate: Yes - portability fix for modern GCC.</p>"},{"location":"reference/modifications/#3-gen_mksurfdata_build-gcc-14-flags","title":"3. gen_mksurfdata_build - GCC 14 Flags","text":"<p>File: <code>tools/mksurfdata_esmf/gen_mksurfdata_build</code></p> <p>Problem: GCC 14 is stricter about legacy Fortran code patterns.</p> <p>Fix: Add compatibility flags:</p> <pre><code>-DCMAKE_Fortran_FLAGS=\" -fallow-argument-mismatch -fallow-invalid-boz -ffree-line-length-none\"\n</code></pre> <p>Flags explained: - <code>-fallow-argument-mismatch</code>: Allow type mismatches in procedure calls - <code>-fallow-invalid-boz</code>: Allow invalid BOZ literal constants - <code>-ffree-line-length-none</code>: No line length limit</p> <p>Contribution candidate: Maybe - these are workarounds, not fixes for underlying issues.</p>"},{"location":"reference/modifications/#4-single_point_casepy-mpilib","title":"4. single_point_case.py - MPILIB","text":"<p>File: <code>python/ctsm/site_and_regional/single_point_case.py</code></p> <p>Change: <code>MPILIB=mpi-serial</code> \u2192 <code>MPILIB=openmpi</code></p> <p>Reason: HiPerGator uses OpenMPI, not mpi-serial.</p> <p>Contribution candidate: No - HiPerGator-specific.</p>"},{"location":"reference/modifications/#5-default_data_cfg-input-paths","title":"5. default_data_*.cfg - Input Paths","text":"<p>Files: <code>tools/site_and_regional/default_data_2000.cfg</code>, <code>default_data_1850.cfg</code></p> <p>Change: NCAR paths \u2192 HiPerGator paths</p> <p>Example: <pre><code># Before (NCAR)\n[main]\nclmforcingindir = /glade/campaign/...\n\n# After (HiPerGator)\n[main]\nclmforcingindir = /blue/&lt;group&gt;/earth_models/inputdata\n</code></pre></p> <p>Contribution candidate: No - site-specific.</p>"},{"location":"reference/modifications/#ccs_config-modifications","title":"ccs_config Modifications","text":""},{"location":"reference/modifications/#machine-configuration","title":"Machine Configuration","text":"<p>Files in <code>machines/hipergator/</code>: - <code>config_machines.xml</code> - Machine definition - <code>config_batch.xml</code> - SLURM settings - <code>gnu_hipergator.cmake</code> - Compiler flags</p> <p>Key changes: - GCC 14.2.0, OpenMPI 5.0.7, ESMF 8.8.1 - Shared PIO path - No <code>--exclusive</code> flag (HiPerGator uses shared nodes) - Group-specific QoS queues (customize for your group)</p>"},{"location":"reference/modifications/#why-fork-ccs_config","title":"Why Fork ccs_config?","text":"<p>The <code>~/.cime/</code> user override approach has CIME v3 bugs: - Batch config loading inconsistent - NODENAME_REGEX detection issues - Each user must maintain configs</p> <p>Forking gives a single source of truth.</p>"},{"location":"reference/modifications/#upstream-contribution-status","title":"Upstream Contribution Status","text":"Modification Status Notes CMakeLists STATIC/SHARED Planned Clear bug Format specifiers Planned Portability fix GCC 14 flags Under review Workarounds vs fixes HiPerGator paths Not applicable Site-specific ccs_config Not applicable Site-specific"},{"location":"reference/modifications/#reference","title":"Reference","text":"<ul> <li>GCC Format Descriptors</li> <li>CESM Forum: PIO Errors</li> </ul>"},{"location":"reference/resources/","title":"External Resources","text":"<p>Curated links to official CTSM documentation and community resources.</p>"},{"location":"reference/resources/#official-documentation","title":"Official Documentation","text":""},{"location":"reference/resources/#ctsm","title":"CTSM","text":"Resource Description CTSM GitHub Wiki Development documentation, workflows, coding guidelines CTSM Technical Note Model science documentation (32 chapters) CTSM User's Guide Setup and running guide CTSM GitHub Source code and issue tracker"},{"location":"reference/resources/#cime-infrastructure","title":"CIME Infrastructure","text":"Resource Description CIME Documentation Case control system documentation CIME Troubleshooting Debugging guide"},{"location":"reference/resources/#cesm-parent-project","title":"CESM (Parent Project)","text":"Resource Description CESM Tutorial Hands-on guides and exercises CESM Documentation Full CESM documentation"},{"location":"reference/resources/#community-resources","title":"Community Resources","text":""},{"location":"reference/resources/#forums","title":"Forums","text":"Resource Description DiscussCESM Community forum for questions CLM Tag CLM-specific discussions"},{"location":"reference/resources/#tutorials-workshops","title":"Tutorials &amp; Workshops","text":"Resource Description CESM Tutorial Materials Workshop slides and exercises CTSM Tutorial CTSM-specific tutorial materials"},{"location":"reference/resources/#key-wiki-pages","title":"Key Wiki Pages","text":""},{"location":"reference/resources/#development","title":"Development","text":"<ul> <li>CTSM Development Workflow</li> <li>Quick Start to CTSM Development</li> <li>CTSM Coding Guidelines</li> <li>CLM Coding Conventions</li> </ul>"},{"location":"reference/resources/#testing","title":"Testing","text":"<ul> <li>System Testing Guide</li> <li>List of Common Problems</li> </ul>"},{"location":"reference/resources/#features","title":"Features","text":"<ul> <li>Adding New Namelist Items</li> <li>Adding Restart Variables</li> <li>Moving Parameters to Params File</li> </ul>"},{"location":"reference/resources/#data-resources","title":"Data Resources","text":""},{"location":"reference/resources/#input-data","title":"Input Data","text":"Resource Description CESM Input Data Global input data information NEON Data NEON tower observations"},{"location":"reference/resources/#output-analysis","title":"Output Analysis","text":"Resource Description NCO Tools NetCDF operators for data manipulation CDO Tools Climate data operators xarray Python library for NetCDF analysis"},{"location":"reference/resources/#hipergator-resources","title":"HiPerGator Resources","text":"Resource Description HiPerGator Docs UF HPC documentation SLURM Guide Job scheduling on HiPerGator Training Videos HiPerGator training materials"},{"location":"reference/resources/#reference-priority","title":"Reference Priority","text":"<p>When looking for information:</p> <ol> <li>Local documentation (this site, hpg-esm-tools)</li> <li>Official CTSM docs (wiki, tech note, user guide)</li> <li>CESM forums (community knowledge)</li> <li>Third-party sources (verify carefully)</li> </ol>"},{"location":"research/hillslope/","title":"Hillslope Hydrology","text":"<p>Documentation for hillslope hydrology research and CTSM modifications.</p> <p>Status: Placeholder - content to be added.</p>"},{"location":"research/hillslope/#overview","title":"Overview","text":""},{"location":"research/hillslope/#hillslope-structure","title":"Hillslope Structure","text":""},{"location":"research/hillslope/#key-parameters","title":"Key Parameters","text":""},{"location":"research/hillslope/#analysis-scripts","title":"Analysis Scripts","text":"<p>Analysis scripts are available in hpg-esm-tools:</p> <pre><code>scripts/hillslope.analysis/\n\u251c\u2500\u2500 bin_temporal.sh           # Temporal binning\n\u251c\u2500\u2500 plot_timeseries_*.py      # Time series plots\n\u251c\u2500\u2500 plot_zwt_hillslope_profile.py  # Water table profiles\n\u251c\u2500\u2500 plot_elevation_width_overlay.py # Hillslope geometry\n\u2514\u2500\u2500 plot_col_areas.py         # Column areas\n</code></pre>"},{"location":"research/hillslope/#history-output","title":"History Output","text":"<p>Relevant variables for hillslope analysis:</p> Variable Description <code>ZWT</code> Water table depth <code>QRUNOFF</code> Total runoff <code>QOVER</code> Surface runoff <code>QDRAI</code> Subsurface drainage <p>See History Output for configuring output.</p>"},{"location":"research/neon-sites/","title":"NEON Sites","text":"<p>Documentation for NEON (National Ecological Observatory Network) site simulations.</p> <p>Status: Placeholder - content to be added.</p>"},{"location":"research/neon-sites/#overview","title":"Overview","text":""},{"location":"research/neon-sites/#sites-of-interest","title":"Sites of Interest","text":"Site Location Ecosystem Status OSBS Florida Longleaf pine Active ... ... ... ..."},{"location":"research/neon-sites/#data-extraction","title":"Data Extraction","text":""},{"location":"research/neon-sites/#subset-data","title":"Subset Data","text":"<p>Pre-generated subset data for NEON sites (example structure):</p> <pre><code>/blue/&lt;group&gt;/earth_models/shared.subset.data/\n\u251c\u2500\u2500 OSBS/\n\u2502   \u251c\u2500\u2500 surfdata_*.nc\n\u2502   \u251c\u2500\u2500 datmdata/\n\u2502   \u2514\u2500\u2500 user_mods/\n\u2514\u2500\u2500 .../\n</code></pre>"},{"location":"research/neon-sites/#configuration-files","title":"Configuration Files","text":"<p>Site-specific configurations in the CTSM fork:</p> <pre><code>$CTSMROOT/tools/site_and_regional/\n\u251c\u2500\u2500 osbs.cfg\n\u2514\u2500\u2500 ...\n</code></pre> <p>See Single-Point Runs for creating site simulations.</p>"},{"location":"research/overview/","title":"Research Overview","text":"<p>This section documents research applications of CTSM on HiPerGator.</p> <p>Status: Placeholder - content to be added.</p>"},{"location":"research/overview/#research-areas","title":"Research Areas","text":""},{"location":"research/overview/#active-projects","title":"Active Projects","text":""},{"location":"research/overview/#publications","title":"Publications","text":"<p>See Hillslope Hydrology and NEON Sites for specific research areas.</p>"},{"location":"running-ctsm/case-workflow/","title":"Case Workflow","text":"<p>CTSM uses a case-based workflow where each simulation is configured, built, and run from a dedicated case directory.</p>"},{"location":"running-ctsm/case-workflow/#overview","title":"Overview","text":"<pre><code>create_newcase \u2192 case.setup \u2192 case.build \u2192 case.submit\n</code></pre> <p>Each step creates or modifies files in the case directory.</p>"},{"location":"running-ctsm/case-workflow/#case-lifecycle","title":"Case Lifecycle","text":""},{"location":"running-ctsm/case-workflow/#1-create-a-new-case","title":"1. Create a New Case","text":"<pre><code>cd $CIME_SCRIPTS\n\n./create_newcase \\\n    --case $CASES/my_case_name \\\n    --compset I1850Clm60BgcCrop \\\n    --res f09_g17 \\\n    --machine hipergator \\\n    --run-unsupported\n</code></pre> <p>Key arguments:</p> Argument Description <code>--case</code> Path to the case directory <code>--compset</code> Component set (model configuration) <code>--res</code> Resolution (grid) <code>--machine</code> Target machine <code>--run-unsupported</code> Required for non-NCAR machines"},{"location":"running-ctsm/case-workflow/#2-configure-the-case","title":"2. Configure the Case","text":"<pre><code>cd $CASES/my_case_name\n\n# Modify XML settings\n./xmlchange STOP_OPTION=nyears\n./xmlchange STOP_N=5\n./xmlchange JOB_WALLCLOCK_TIME=04:00:00\n\n# Query current values\n./xmlquery STOP_OPTION STOP_N\n</code></pre> <p>Common XML variables:</p> Variable Description Example <code>STOP_OPTION</code> Time units <code>nyears</code>, <code>nmonths</code>, <code>ndays</code> <code>STOP_N</code> Number of time units <code>5</code> <code>JOB_WALLCLOCK_TIME</code> SLURM time limit <code>04:00:00</code> <code>NTASKS</code> Number of MPI tasks <code>32</code>"},{"location":"running-ctsm/case-workflow/#3-setup-the-case","title":"3. Setup the Case","text":"<pre><code>./case.setup\n</code></pre> <p>This creates: - Namelist files (<code>user_nl_clm</code>, etc.) - Build scripts - Run scripts</p>"},{"location":"running-ctsm/case-workflow/#4-modify-namelists","title":"4. Modify Namelists","text":"<p>Edit <code>user_nl_clm</code> to customize model behavior:</p> <pre><code>! Example: Configure history output\nhist_empty_htapes = .true.\nhist_fincl1 = 'TSA', 'GPP', 'EFLX_LH_TOT'\nhist_nhtfrq = -24\nhist_mfilt = 365\n</code></pre>"},{"location":"running-ctsm/case-workflow/#5-build-the-case","title":"5. Build the Case","text":"<pre><code>./case.build\n</code></pre> <p>This compiles the model. Takes 10-30 minutes depending on configuration.</p> <p>Rebuilding: If you change Fortran source code or XML build variables: <pre><code>./case.build --clean-all\n./case.build\n</code></pre></p>"},{"location":"running-ctsm/case-workflow/#6-submit-the-case","title":"6. Submit the Case","text":"<pre><code>./case.submit\n</code></pre> <p>This submits the job to SLURM. Monitor with: <pre><code>squeue -u $USER\n</code></pre></p>"},{"location":"running-ctsm/case-workflow/#directory-structure","title":"Directory Structure","text":"<p>After setup, each case has these key directories:</p> Directory Variable Purpose Case root <code>CASEROOT</code> Configuration, scripts, namelists Build <code>EXEROOT</code> Compiled executables Run <code>RUNDIR</code> Output, restarts, logs Archive <code>DOUT_S_ROOT</code> Archived output <p>Query paths: <pre><code>./xmlquery EXEROOT RUNDIR DOUT_S_ROOT\n</code></pre></p>"},{"location":"running-ctsm/case-workflow/#common-compsets","title":"Common Compsets","text":"Compset Description <code>I1850Clm60BgcCrop</code> Pre-industrial BGC with crops <code>I2000Clm60BgcCrop</code> Present-day BGC with crops <code>I1850Clm60Sp</code> Pre-industrial satellite phenology <code>IHistClm60BgcCrop</code> Historical transient run <p>List available compsets: <pre><code>./query_config --compsets clm\n</code></pre></p>"},{"location":"running-ctsm/case-workflow/#common-resolutions","title":"Common Resolutions","text":"Resolution Description <code>f09_g17</code> ~1\u00b0 atmosphere, ~1\u00b0 ocean <code>f19_g17</code> ~2\u00b0 atmosphere, ~1\u00b0 ocean <code>CLM_USRDAT</code> User-provided data (single-point)"},{"location":"running-ctsm/case-workflow/#checking-case-status","title":"Checking Case Status","text":"<p>The <code>CaseStatus</code> file logs all workflow steps:</p> <pre><code>cat CaseStatus\n</code></pre> <p>Look for timestamps and success/failure messages. When a case fails, CaseStatus points to the relevant log file.</p>"},{"location":"running-ctsm/case-workflow/#run-types","title":"Run Types","text":"<p>CTSM supports three run types:</p> Type Use Case Configuration Changes? Bit-for-bit? startup Fresh simulation N/A N/A branch Exact continuation No Yes hybrid New start from restart Yes No"},{"location":"running-ctsm/case-workflow/#continuing-a-run-continue_run","title":"Continuing a Run (CONTINUE_RUN)","text":"<p>To continue a run from where it stopped:</p> <pre><code>./xmlchange CONTINUE_RUN=TRUE\n./case.submit\n</code></pre> <p>This is the simplest continuation - the run picks up exactly where it left off.</p>"},{"location":"running-ctsm/case-workflow/#branch-runs","title":"Branch Runs","text":"<p>Branch runs continue from another case's restart, maintaining bit-for-bit reproducibility:</p> <pre><code>cd $CIME_SCRIPTS\n\n# Create new case\n./create_newcase \\\n    --case $CASES/my_branch_case \\\n    --compset I1850Clm60BgcCrop \\\n    --res f09_g17 \\\n    --machine hipergator \\\n    --run-unsupported\n\ncd $CASES/my_branch_case\n\n# Configure as branch run\n./xmlchange RUN_TYPE=branch\n./xmlchange RUN_REFCASE=source_case_name\n./xmlchange RUN_REFDATE=0005-01-01\n./xmlchange RUN_REFDIR=/path/to/source/case/run\n\n# Copy restart files (if not in RUN_REFDIR)\n./xmlchange GET_REFCASE=FALSE\n\n./case.setup\n./case.build\n./case.submit\n</code></pre> <p>Branch Run Restrictions</p> <p>Branch runs must use the same compset and resolution as the source case. You cannot change model configuration in a branch run.</p>"},{"location":"running-ctsm/case-workflow/#hybrid-runs","title":"Hybrid Runs","text":"<p>Hybrid runs initialize from restart files but allow configuration changes:</p> <pre><code>./xmlchange RUN_TYPE=hybrid\n./xmlchange RUN_REFCASE=source_case_name\n./xmlchange RUN_REFDATE=0005-01-01\n./xmlchange GET_REFCASE=FALSE\n</code></pre> <p>Use hybrid runs when you need to: - Change compset or resolution - Modify namelist settings that affect initialization - Start a new experiment from a spun-up state</p>"},{"location":"running-ctsm/case-workflow/#tips","title":"Tips","text":"<ul> <li>Naming convention: Include compset and date in case names (e.g., <code>I1850Clm60BgcCrop.f09_g17.240115</code>)</li> <li>Check before building: Use <code>./preview_namelists</code> to verify configuration</li> <li>Don't edit generated files: Modify <code>user_nl_*</code> files, not the generated <code>*_in</code> files</li> <li>Keep case directories: They're small and useful for reproducing runs</li> </ul>"},{"location":"running-ctsm/history-output/","title":"History Output","text":"<p>CTSM writes diagnostic output to history files (NetCDF format). This page covers configuring what variables are output, at what frequency, and how they're averaged.</p>"},{"location":"running-ctsm/history-output/#configuration","title":"Configuration","text":"<p>History output is configured in <code>user_nl_clm</code>. Changes take effect on the next run (no rebuild required).</p>"},{"location":"running-ctsm/history-output/#basic-example","title":"Basic Example","text":"<pre><code>! Clear default output, add specific variables\nhist_empty_htapes = .true.\nhist_fincl1 = 'TSA', 'TSKIN', 'GPP', 'EFLX_LH_TOT', 'FSH'\n\n! Daily output, one year per file\nhist_nhtfrq = -24\nhist_mfilt = 365\n</code></pre>"},{"location":"running-ctsm/history-output/#key-namelist-variables","title":"Key Namelist Variables","text":"Variable Purpose Default <code>hist_empty_htapes</code> Clear default output <code>.false.</code> <code>hist_fincl1</code> - <code>hist_fincl10</code> Variables for h0-h9 streams varies <code>hist_nhtfrq</code> Output frequency <code>0</code> (monthly) <code>hist_mfilt</code> Samples per file <code>1</code>"},{"location":"running-ctsm/history-output/#output-frequency","title":"Output Frequency","text":"<p>The <code>hist_nhtfrq</code> variable controls output frequency:</p> Value Meaning Example <code>0</code> Monthly average Default <code>&gt; 0</code> Every N timesteps <code>48</code> = every 48 timesteps <code>&lt; 0</code> Every N hours <code>-24</code> = daily, <code>-1</code> = hourly"},{"location":"running-ctsm/history-output/#examples","title":"Examples","text":"<pre><code>! Monthly averages (default)\nhist_nhtfrq = 0\n\n! Daily averages\nhist_nhtfrq = -24\n\n! 6-hourly output\nhist_nhtfrq = -6\n\n! Every timestep (30-min default)\nhist_nhtfrq = 1\n</code></pre>"},{"location":"running-ctsm/history-output/#samples-per-file","title":"Samples Per File","text":"<p>The <code>hist_mfilt</code> variable sets how many time samples go in each file:</p> <pre><code>! One year of daily data per file\nhist_nhtfrq = -24\nhist_mfilt = 365\n\n! One month of daily data per file\nhist_nhtfrq = -24\nhist_mfilt = 31\n\n! 12 months per file (monthly data)\nhist_nhtfrq = 0\nhist_mfilt = 12\n</code></pre>"},{"location":"running-ctsm/history-output/#averaging-flags","title":"Averaging Flags","text":"<p>By default, variables are averaged over the output period. Append a flag to change this:</p> Flag Meaning Example <code>A</code> Average (default) <code>'GPP'</code> or <code>'GPP:A'</code> <code>I</code> Instantaneous <code>'TSA:I'</code> <code>M</code> Minimum <code>'TSA:M'</code> <code>X</code> Maximum <code>'TSA:X'</code> <code>SUM</code> Sum <code>'RAIN:SUM'</code> <pre><code>! Daily max and min temperature\nhist_fincl1 = 'TSA:X', 'TSA:M', 'GPP'\n</code></pre>"},{"location":"running-ctsm/history-output/#multiple-history-streams","title":"Multiple History Streams","text":"<p>You can configure up to 10 independent history streams (h0-h9):</p> <pre><code>! h0: Monthly averages of key variables\nhist_fincl1 = 'GPP', 'NPP', 'NEE', 'TLAI'\nhist_nhtfrq(1) = 0\nhist_mfilt(1) = 12\n\n! h1: Daily water balance\nhist_fincl2 = 'QSOIL', 'QVEGE', 'QVEGT', 'RAIN', 'SNOW'\nhist_nhtfrq(2) = -24\nhist_mfilt(2) = 365\n\n! h2: Hourly temperature for heat waves\nhist_fincl3 = 'TSA', 'TSKIN'\nhist_nhtfrq(3) = -1\nhist_mfilt(3) = 24\n</code></pre>"},{"location":"running-ctsm/history-output/#common-variables","title":"Common Variables","text":""},{"location":"running-ctsm/history-output/#energy-balance","title":"Energy Balance","text":"Variable Description Units <code>FSH</code> Sensible heat flux W/m\u00b2 <code>EFLX_LH_TOT</code> Latent heat flux W/m\u00b2 <code>FSA</code> Absorbed solar radiation W/m\u00b2 <code>FIRA</code> Net infrared radiation W/m\u00b2"},{"location":"running-ctsm/history-output/#carbon-fluxes","title":"Carbon Fluxes","text":"Variable Description Units <code>GPP</code> Gross primary production gC/m\u00b2/s <code>NPP</code> Net primary production gC/m\u00b2/s <code>NEE</code> Net ecosystem exchange gC/m\u00b2/s <code>ER</code> Ecosystem respiration gC/m\u00b2/s"},{"location":"running-ctsm/history-output/#hydrology","title":"Hydrology","text":"Variable Description Units <code>QRUNOFF</code> Total runoff mm/s <code>QSOIL</code> Ground evaporation mm/s <code>QVEGE</code> Canopy evaporation mm/s <code>QVEGT</code> Canopy transpiration mm/s <code>TWS</code> Total water storage mm <code>ZWT</code> Water table depth m"},{"location":"running-ctsm/history-output/#temperature","title":"Temperature","text":"Variable Description Units <code>TSA</code> 2m air temperature K <code>TSKIN</code> Surface temperature K <code>TSOI</code> Soil temperature K <code>TV</code> Vegetation temperature K"},{"location":"running-ctsm/history-output/#vegetation","title":"Vegetation","text":"Variable Description Units <code>TLAI</code> Total LAI m\u00b2/m\u00b2 <code>TOTVEGC</code> Total vegetation carbon gC/m\u00b2 <code>TOTSOMC</code> Total soil organic carbon gC/m\u00b2"},{"location":"running-ctsm/history-output/#finding-available-variables","title":"Finding Available Variables","text":"<p>List all available history variables:</p> <pre><code># Search the history field definitions\ngrep -r \"hist_\" $CTSMROOT/bld/namelist_files/\n</code></pre> <p>Or check the CTSM documentation: History Fields</p>"},{"location":"running-ctsm/history-output/#output-file-naming","title":"Output File Naming","text":"<p>History files follow this naming convention:</p> <pre><code>&lt;case_name&gt;.clm2.h0.YYYY-MM.nc    # Monthly h0 stream\n&lt;case_name&gt;.clm2.h1.YYYY-MM-DD.nc # Daily h1 stream\n</code></pre>"},{"location":"running-ctsm/history-output/#tips","title":"Tips","text":"<ul> <li>Start minimal: Begin with <code>hist_empty_htapes = .true.</code> and add only what you need</li> <li>Check file sizes: High-frequency output can generate large files</li> <li>Use multiple streams: Separate high-frequency and low-frequency output</li> <li>Document your choices: Note what variables you're using and why</li> </ul>"},{"location":"running-ctsm/single-point/","title":"Single-Point Runs","text":"<p>Single-point (site-level) simulations allow you to run CTSM for a specific location rather than a global grid. This is useful for:</p> <ul> <li>Validating against tower observations (NEON, FLUXNET)</li> <li>Testing model changes quickly</li> <li>Site-specific research</li> </ul>"},{"location":"running-ctsm/single-point/#subset-data-workflow","title":"Subset Data Workflow","text":"<p>CTSM provides the <code>subset_data</code> tool to extract site-level data from global datasets.</p>"},{"location":"running-ctsm/single-point/#overview","title":"Overview","text":"<pre><code>subset_data \u2192 creates site data \u2192 create_newcase with CLM_USRDAT\n</code></pre> <p>The subset data creates: - Surface data for the site - Atmospheric forcing data (DATM) - User mods for case configuration</p>"},{"location":"running-ctsm/single-point/#creating-subset-data","title":"Creating Subset Data","text":"<pre><code>cd $CTSMROOT/tools/site_and_regional\n\n# Activate CTSM Python environment\nmodule load conda\nconda activate ctsm_pylib\n\n# Run subset_data for a site\n./subset_data point \\\n    --lat 29.6893 \\\n    --lon -82.0096 \\\n    --site OSBS \\\n    --create-surface \\\n    --create-datm \\\n    --datm-syr 2000 \\\n    --datm-eyr 2020 \\\n    --outdir $SUBSET_DATA/OSBS\n</code></pre> <p>Key arguments:</p> Argument Description <code>--lat</code>, <code>--lon</code> Site coordinates <code>--site</code> Site name (used in filenames) <code>--create-surface</code> Generate surface data <code>--create-datm</code> Generate atmospheric forcing <code>--datm-syr</code>, <code>--datm-eyr</code> Forcing data year range <code>--outdir</code> Output directory"},{"location":"running-ctsm/single-point/#subset-data-output","title":"Subset Data Output","text":"<p>The tool creates a directory structure:</p> <pre><code>$SUBSET_DATA/OSBS/\n\u251c\u2500\u2500 surfdata_*.nc           # Surface dataset\n\u251c\u2500\u2500 datmdata/               # Atmospheric forcing\n\u2502   \u2514\u2500\u2500 atm_forcing.*.nc\n\u2514\u2500\u2500 user_mods/\n    \u251c\u2500\u2500 shell_commands      # XML changes\n    \u251c\u2500\u2500 user_nl_clm         # CLM namelist additions\n    \u2514\u2500\u2500 user_nl_datm_streams # DATM configuration\n</code></pre>"},{"location":"running-ctsm/single-point/#creating-a-case-with-subset-data","title":"Creating a Case with Subset Data","text":"<pre><code>cd $CIME_SCRIPTS\n\n./create_newcase \\\n    --case $CASES/OSBS.I1850Clm60BgcCrop \\\n    --compset I1850Clm60BgcCrop \\\n    --res CLM_USRDAT \\\n    --machine hipergator \\\n    --run-unsupported \\\n    --user-mods-dirs $SUBSET_DATA/OSBS/user_mods\n</code></pre> <p>Key difference: Use <code>--res CLM_USRDAT</code> and <code>--user-mods-dirs</code> to point to your subset data.</p>"},{"location":"running-ctsm/single-point/#configure-and-run","title":"Configure and Run","text":"<pre><code>cd $CASES/OSBS.I1850Clm60BgcCrop\n\n# Verify settings (user_mods should have applied changes)\n./xmlquery CLM_USRDAT_NAME DATM_MODE\n\n# Setup and build\n./case.setup\n./case.build\n\n# Modify run length as needed\n./xmlchange STOP_OPTION=nyears\n./xmlchange STOP_N=5\n./xmlchange JOB_WALLCLOCK_TIME=02:00:00\n\n# Submit\n./case.submit\n</code></pre>"},{"location":"running-ctsm/single-point/#available-forcing-data","title":"Available Forcing Data","text":"<p>Our fork is configured for CRUNCEP forcing:</p> Dataset Years Description CRUNCEPv7 1901-2016 6-hourly reanalysis <p>The <code>default_data_*.cfg</code> files in the fork specify HiPerGator paths.</p>"},{"location":"running-ctsm/single-point/#neon-sites","title":"NEON Sites","text":"<p>For NEON tower sites, CTSM provides pre-configured options:</p> <pre><code># List available NEON sites\n./subset_data point --help\n</code></pre> <p>Common sites:</p> Site Location Ecosystem OSBS Florida Longleaf pine HARV Massachusetts Mixed deciduous BART New Hampshire Northern hardwood"},{"location":"running-ctsm/single-point/#shared-subset-data","title":"Shared Subset Data","text":"<p>Your group may maintain pre-generated subset data for common sites. Check with your group before generating new ones - subset data can be shared to avoid duplicate downloads.</p> <p>Typical location: <code>/blue/&lt;group&gt;/earth_models/shared.subset.data/</code></p>"},{"location":"running-ctsm/single-point/#troubleshooting","title":"Troubleshooting","text":""},{"location":"running-ctsm/single-point/#subset_data-errors","title":"subset_data Errors","text":"Error Cause Solution \"No data found\" Coordinates outside data range Verify lat/lon Import errors Missing Python packages Activate <code>ctsm_pylib</code> Path errors Wrong input data location Check <code>$INPUT_DATA</code>"},{"location":"running-ctsm/single-point/#case-errors","title":"Case Errors","text":"Error Cause Solution \"CLM_USRDAT_NAME not set\" user_mods not applied Check <code>--user-mods-dirs</code> path Missing surface data File not found Verify <code>$SUBSET_DATA</code> path"},{"location":"running-ctsm/single-point/#tips","title":"Tips","text":"<ul> <li>Start small: Run a short test (1 year) before long simulations</li> <li>Check user_mods: After <code>case.setup</code>, verify the settings were applied</li> <li>Keep subset data: It's faster to reuse than regenerate</li> <li>Coordinate precision: Use enough decimal places for your site</li> </ul>"},{"location":"running-ctsm/single-point/#pts_mode-deprecated","title":"PTS_MODE (Deprecated)","text":"<p>Older documentation may reference <code>PTS_MODE</code> for single-point runs. This is deprecated in CTSM and has limitations (no restart capability). Use the subset data workflow instead.</p>"},{"location":"running-ctsm/spinup/","title":"Spinup","text":"<p>BGC (biogeochemistry) simulations require spinup to bring soil carbon pools to equilibrium. This process can take hundreds to thousands of simulated years.</p>"},{"location":"running-ctsm/spinup/#why-spinup","title":"Why Spinup?","text":"<p>Carbon pools in soil take centuries to equilibrate. Starting from arbitrary initial conditions would produce unrealistic results. Spinup runs the model repeatedly until carbon pools stabilize.</p>"},{"location":"running-ctsm/spinup/#two-phase-spinup","title":"Two-Phase Spinup","text":"<p>CTSM uses a two-phase approach:</p>"},{"location":"running-ctsm/spinup/#phase-1-accelerated-decomposition-ad-spinup","title":"Phase 1: Accelerated Decomposition (AD) Spinup","text":"<p>Artificially accelerates soil carbon turnover to reach approximate equilibrium faster.</p> <pre><code>./xmlchange CLM_ACCELERATED_SPINUP=on\n</code></pre> <p>Duration: ~200 years (can vary by site)</p>"},{"location":"running-ctsm/spinup/#phase-2-post-ad-spinup","title":"Phase 2: Post-AD Spinup","text":"<p>Returns to normal decomposition rates and runs to final equilibrium.</p> <pre><code>./xmlchange CLM_ACCELERATED_SPINUP=off\n</code></pre> <p>Duration: Several hundred years</p>"},{"location":"running-ctsm/spinup/#spinup-workflow","title":"Spinup Workflow","text":""},{"location":"running-ctsm/spinup/#1-create-ad-spinup-case","title":"1. Create AD Spinup Case","text":"<pre><code>cd $CIME_SCRIPTS\n\n./create_newcase \\\n    --case $CASES/my_site.AD_spinup \\\n    --compset I1850Clm60BgcCrop \\\n    --res CLM_USRDAT \\\n    --machine hipergator \\\n    --run-unsupported \\\n    --user-mods-dirs $SUBSET_DATA/my_site/user_mods\n\ncd $CASES/my_site.AD_spinup\n\n# Enable accelerated spinup\n./xmlchange CLM_ACCELERATED_SPINUP=on\n\n# Run for 200 years\n./xmlchange STOP_OPTION=nyears\n./xmlchange STOP_N=50\n./xmlchange RESUBMIT=3  # 4 submissions \u00d7 50 years = 200 years\n\n./case.setup\n./case.build\n./case.submit\n</code></pre>"},{"location":"running-ctsm/spinup/#2-monitor-ad-spinup","title":"2. Monitor AD Spinup","text":"<p>Check that carbon pools are trending toward equilibrium:</p> <pre><code># Look at TOTSOMC (soil organic carbon) over time\nncdump -v TOTSOMC $RUNDIR/*.clm2.h0.*.nc | tail\n</code></pre>"},{"location":"running-ctsm/spinup/#3-create-post-ad-case","title":"3. Create Post-AD Case","text":"<pre><code>cd $CIME_SCRIPTS\n\n./create_newcase \\\n    --case $CASES/my_site.postAD_spinup \\\n    --compset I1850Clm60BgcCrop \\\n    --res CLM_USRDAT \\\n    --machine hipergator \\\n    --run-unsupported \\\n    --user-mods-dirs $SUBSET_DATA/my_site/user_mods\n\ncd $CASES/my_site.postAD_spinup\n\n# Hybrid start from AD spinup\n./xmlchange RUN_TYPE=hybrid\n./xmlchange RUN_REFCASE=my_site.AD_spinup\n./xmlchange RUN_REFDATE=0201-01-01  # End of AD spinup\n./xmlchange GET_REFCASE=FALSE\n\n# Copy restart files\ncp $AD_RUNDIR/*.rpointer* $RUNDIR/\ncp $AD_RUNDIR/*.r.*.nc $RUNDIR/\n\n# Normal decomposition\n./xmlchange CLM_ACCELERATED_SPINUP=off\n\n# Run several hundred years\n./xmlchange STOP_OPTION=nyears\n./xmlchange STOP_N=100\n./xmlchange RESUBMIT=4  # 500 years total\n\n./case.setup\n./case.build\n./case.submit\n</code></pre>"},{"location":"running-ctsm/spinup/#4-check-equilibrium","title":"4. Check Equilibrium","text":"<p>Spinup is complete when: - Less than ~3% of land surface shows carbon disequilibrium - TOTSOMC, TOTVEGC, TOTECOSYSC are stable year-to-year</p>"},{"location":"running-ctsm/spinup/#variables-to-monitor","title":"Variables to Monitor","text":"Variable Description <code>TOTECOSYSC</code> Total ecosystem carbon <code>TOTSOMC</code> Soil organic matter carbon <code>TOTVEGC</code> Vegetation carbon <code>GPP</code> Gross primary production <code>TWS</code> Total water storage <p>Include these in your history output:</p> <pre><code>hist_fincl1 = 'TOTECOSYSC', 'TOTSOMC', 'TOTVEGC', 'GPP', 'TWS'\nhist_nhtfrq = 0  ! Monthly\nhist_mfilt = 12  ! 12 months per file\n</code></pre>"},{"location":"running-ctsm/spinup/#equilibrium-criteria","title":"Equilibrium Criteria","text":"<p>The model reaches equilibrium when carbon fluxes balance:</p> <ul> <li>NEE \u2248 0 (net ecosystem exchange)</li> <li>Year-to-year changes in carbon pools &lt; 0.1%</li> </ul> <p>Arctic and boreal regions take longest (~1000 years).</p>"},{"location":"running-ctsm/spinup/#tips","title":"Tips","text":"<ul> <li>Start with AD: Never skip AD spinup for BGC runs</li> <li>Check frequently: Don't run hundreds of years without checking progress</li> <li>Save restarts: Keep restart files at key points</li> <li>Site-specific duration: Some sites equilibrate faster than others</li> <li>SP mode doesn't need spinup: Satellite phenology runs don't have prognostic carbon</li> </ul>"},{"location":"running-ctsm/spinup/#common-issues","title":"Common Issues","text":"Issue Cause Solution Carbon still drifting Not enough spinup time Continue running Oscillating values Numerical issues Check for extreme climate Very slow equilibration Cold/wet sites Normal for arctic/boreal"},{"location":"running-ctsm/spinup/#satellite-phenology-sp-mode","title":"Satellite Phenology (SP) Mode","text":"<p>If you don't need prognostic carbon pools, SP mode skips the carbon cycle entirely:</p> <pre><code>--compset I1850Clm60Sp  # No BGC, no spinup needed\n</code></pre> <p>SP runs use prescribed LAI from satellite observations and don't require spinup.</p>"},{"location":"running-ctsm/troubleshooting/","title":"Troubleshooting","text":"<p>Guide to diagnosing and fixing common CTSM failures on HiPerGator.</p>"},{"location":"running-ctsm/troubleshooting/#first-steps","title":"First Steps","text":"<p>When a case fails:</p> <ol> <li> <p>Check CaseStatus - The <code>CaseStatus</code> file in your case directory logs all workflow steps with timestamps. Look for error messages and paths to log files.</p> </li> <li> <p>Follow the path - CaseStatus points to the specific log containing the error. Read that file.</p> </li> <li> <p>Check SLURM output - Look for <code>slurm-*.out</code> files in the case directory.</p> </li> </ol> <pre><code>cd $CASES/my_case\ncat CaseStatus | tail -20\n</code></pre>"},{"location":"running-ctsm/troubleshooting/#slurm-failures","title":"SLURM Failures","text":""},{"location":"running-ctsm/troubleshooting/#node_fail","title":"NODE_FAIL","text":"<p>Message: <code>Job &lt;job_id&gt; failed, node failure</code></p> <p>Cause: Hardware/system failure on compute node.</p> <p>Solution: Resubmit the job. If persistent, contact HiPerGator support.</p>"},{"location":"running-ctsm/troubleshooting/#timeout","title":"TIMEOUT","text":"<p>Message: <code>Job &lt;job_id&gt; cancelled at &lt;timestamp&gt; because it expired</code></p> <p>Cause: Job exceeded wall time limit.</p> <p>Solution: <pre><code>./xmlchange JOB_WALLCLOCK_TIME=08:00:00\n./case.submit\n</code></pre></p> <p>Or reduce run length with <code>STOP_N</code>.</p>"},{"location":"running-ctsm/troubleshooting/#out_of_memory","title":"OUT_OF_MEMORY","text":"<p>Message: <code>Job &lt;job_id&gt; killed due to out-of-memory</code></p> <p>Cause: Exceeded memory allocation.</p> <p>Solution: Request more memory or reduce domain size.</p>"},{"location":"running-ctsm/troubleshooting/#failed-exit-code","title":"FAILED (Exit Code)","text":"<p>Message: <code>Job &lt;job_id&gt; failed with exit code &lt;X&gt;</code></p> <p>Cause: Model crash - segfault, missing file, numerical error.</p> <p>Solution: Check the component log files (see below).</p>"},{"location":"running-ctsm/troubleshooting/#log-file-analysis","title":"Log File Analysis","text":""},{"location":"running-ctsm/troubleshooting/#finding-log-files","title":"Finding Log Files","text":"<pre><code># Query run directory\n./xmlquery RUNDIR\n\n# List logs by modification time (most recent first)\nls -t $RUNDIR/*.log.*\n</code></pre>"},{"location":"running-ctsm/troubleshooting/#key-log-files","title":"Key Log Files","text":"Log Contains <code>cesm.log.*</code> Top-level model output <code>lnd.log.*</code> Land (CLM) component messages <code>cpl.log.*</code> Coupler output, timing info <code>atm.log.*</code> Atmosphere (DATM) messages"},{"location":"running-ctsm/troubleshooting/#searching-for-errors","title":"Searching for Errors","text":"<pre><code># Search all logs for errors\ncd $RUNDIR\ngrep -i \"error\\|abort\\|fail\" *.log.* | head -50\n\n# Check end of land log\ntail -100 lnd.log.*\n</code></pre>"},{"location":"running-ctsm/troubleshooting/#did-it-fail-or-time-out","title":"Did It Fail or Time Out?","text":"<p>Check if all log files were modified at the same time:</p> <pre><code>cd $RUNDIR\nstat *.log.* | grep Modify\n</code></pre> <p>If times are within seconds of each other, it likely timed out (the job was killed while running). If times vary, the model crashed.</p>"},{"location":"running-ctsm/troubleshooting/#common-model-errors","title":"Common Model Errors","text":""},{"location":"running-ctsm/troubleshooting/#missing-input-files","title":"Missing Input Files","text":"<p>Symptom: Error about file not found in <code>lnd.log</code></p> <p>Solution: Check paths in namelists, verify input data exists: <pre><code>ls $INPUT_DATA/path/to/file\n</code></pre></p>"},{"location":"running-ctsm/troubleshooting/#namelist-errors","title":"Namelist Errors","text":"<p>Symptom: Error parsing namelist in <code>lnd.log</code></p> <p>Solution: Check <code>user_nl_clm</code> for syntax errors. Run: <pre><code>./preview_namelists\n</code></pre></p>"},{"location":"running-ctsm/troubleshooting/#numerical-instability","title":"Numerical Instability","text":"<p>Symptom: NaN or Inf values, model crashes mid-run</p> <p>Solution: - Check forcing data for extreme values - Try reducing timestep - Enable debug mode for more info</p>"},{"location":"running-ctsm/troubleshooting/#memory-issues","title":"Memory Issues","text":"<p>Symptom: Segfault, out-of-memory errors</p> <p>Solution: - Reduce domain size - Increase memory allocation - Check for array bounds issues (enable DEBUG=TRUE)</p>"},{"location":"running-ctsm/troubleshooting/#debugging-mode","title":"Debugging Mode","text":""},{"location":"running-ctsm/troubleshooting/#runtime-diagnostics-no-rebuild","title":"Runtime Diagnostics (No Rebuild)","text":"<pre><code>./xmlchange INFO_DBUG=2\n./case.submit\n</code></pre> <p>Adds diagnostic output to <code>cpl.log</code>.</p>"},{"location":"running-ctsm/troubleshooting/#full-debug-mode-requires-rebuild","title":"Full Debug Mode (Requires Rebuild)","text":"<pre><code>./xmlchange DEBUG=TRUE\n./case.build --clean-all\n./case.build\n./case.submit\n</code></pre> <p>Enables bounds checking, floating-point exception trapping. Runs much slower.</p>"},{"location":"running-ctsm/troubleshooting/#build-failures","title":"Build Failures","text":""},{"location":"running-ctsm/troubleshooting/#compilation-errors","title":"Compilation Errors","text":"<p>Check the build log: <pre><code>./xmlquery EXEROOT\ncat $EXEROOT/lnd.bldlog.*\n</code></pre></p> <p>Common causes: - Missing modules (run <code>module restore ctsm-modules</code>) - Syntax errors in SourceMods</p>"},{"location":"running-ctsm/troubleshooting/#rebuild-after-changes","title":"Rebuild After Changes","text":"<p>If you modify source code or build settings: <pre><code>./case.build --clean-all\n./case.build\n</code></pre></p>"},{"location":"running-ctsm/troubleshooting/#timing-analysis","title":"Timing Analysis","text":"<p>Check simulation performance in <code>cpl.log</code>:</p> <pre><code>grep tStamp cpl.log.* | tail -20\n</code></pre> <p>Output shows time per model day: <pre><code>tStamp_write: model date = 10120 0 wall clock = 2024-01-15 09:10:46 avg dt = 58.58 dt = 58.18\n</code></pre></p> <ul> <li><code>avg dt</code> = average seconds per model day</li> <li><code>dt</code> = seconds for this specific day</li> </ul> <p>Large variations in <code>dt</code> suggest system performance issues.</p>"},{"location":"running-ctsm/troubleshooting/#restart-file-issues","title":"Restart File Issues","text":""},{"location":"running-ctsm/troubleshooting/#corrupted-restarts","title":"Corrupted Restarts","text":"<p>If a job timed out while writing restarts, files may be corrupted.</p> <p>Solution: 1. Check restart file sizes (should be consistent) 2. Remove corrupted files 3. Copy previous restart set 4. Continue from earlier point</p> <pre><code># List restart files by size\nls -la $RUNDIR/*.r.*.nc\n\n# Remove potentially corrupted latest set\nrm $RUNDIR/*.r.0050-01-01*.nc\n\n# Update rpointer files to point to earlier restart\n</code></pre>"},{"location":"running-ctsm/troubleshooting/#getting-help","title":"Getting Help","text":"<ol> <li> <p>Check official docs: CIME Troubleshooting</p> </li> <li> <p>Search forums: DiscussCESM</p> </li> <li> <p>Include in bug reports:</p> </li> <li>CaseStatus file</li> <li>Relevant log excerpts</li> <li>Machine and version info</li> <li>Steps to reproduce</li> </ol>"}]}